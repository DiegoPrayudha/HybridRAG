{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NKQ88SRT5ZZ"
      },
      "outputs": [],
      "source": [
        "pip install -U langchain langchain_community langchain_groq langchain_experimental langchain_core tiktoken rank_bm25 pypdf langchain_chroma langchain_huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTWYX6VjR7sG"
      },
      "source": [
        "# **Load LIB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SIWOItGpR7N_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.schema import Document\n",
        "\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_6p-Mv5FsNS"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9gNDvtAFsCT",
        "outputId": "de16b0a9-8178-4762-c308-cb05533ea841"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7d7ce05511d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7d7ce054e150>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv(\".env\")\n",
        "api_key = os.getenv(\"API_KEY\")\n",
        "\n",
        "model_llm = ChatGroq(groq_api_key=api_key,model_name=\"llama-3.1-8b-instant\")\n",
        "model_llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpVtprVF4BG"
      },
      "source": [
        "# **Load Embedding Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473,
          "referenced_widgets": [
            "1f2bceea54ff4d8aa6283c4e6c41ddc1",
            "8371984b116149429a4506becb3fbae6",
            "90be61d6650d496584140672f5bbdafb",
            "a83eced765f14741a1347c91444e94e7",
            "7acb54d64b124184b143e644676f6e38",
            "8c1c9e62a01748f386a6b8e1ec91135f",
            "37d52bb28ab84ac0a7023ed1f23b9a9a",
            "9ac313c326324d15b5ba259879381fac",
            "c14f850a6d3a4d58885f677a30a93906",
            "ebe3f13acc464184b1bbc1c58d614122",
            "47d20c260a25404c9e8b5a68d043a372",
            "00c46469b578408aaa0dbbe6fc660a20",
            "f34c7fcf71a04f3187040668600b4c91",
            "25782e88330049f99833a1248608551c",
            "59c02a0a307a4474ad112750fb8723f7",
            "89fc44c2d5e14c7cab1017ea7ef97468",
            "19f6ec74cc83420b8cdb52f2bcd8c927",
            "664c3d16230a48a9b29104c41b6c398e",
            "d07b7f893b26422ca85d4554505283c6",
            "60e8326217b740899977d8d189315a65",
            "6ce1e6d73a674b7d950d52999a8ca98e",
            "67a734d3e42445c381e30f1462fbbd5a",
            "fb3f76fbe5e84b3697a91ac81c275c7a",
            "61aa8fce7a8945d59b6f85b3f5d056e0",
            "e87279cf86994427b32d011e116ee537",
            "3dce8e53dba444509931cd9023722480",
            "7543674f08fa470fb2613ca3442d62f7",
            "e4ee15a82dfd4184b6258bd697851c71",
            "35d4b78fb7e64dce9ad10bd9ad02f428",
            "e42b2ba9dede4206821ad2118dd994f6",
            "0b6b60c18373447a80e2c1bb63bf97d2",
            "269c5f04b6c04b08b16abf3e1b807499",
            "b03862c25c4c41debf9184e8585160a7",
            "8426aeaefca047269d5203d205236b76",
            "d565218f0bda47e590ca81850496f795",
            "6a8af57ce8d7487785477a98ad70bd71",
            "99e25b9814e34245ac47e6c67fdb51cf",
            "d2a03ad1d93645b6baee7b6f56c195b3",
            "4a45a5dc16da400c9c0407cda429a28b",
            "39ee33bcfda2446d9dd57736eb6ede85",
            "3a5ebf33451443709348180a2608a779",
            "d93c17258807476eaad0f2ab5fcee4f2",
            "26c3ca12ce8a48fea746e9c8b2c82799",
            "f1f526b078ee4d438ac7a45639eef6a6",
            "e7acc65659a54c8d9f97d808215b1460",
            "4a1c91c87b0344329c43ef47f3f650c2",
            "783556b30c6144259d4b896365eb5ade",
            "558a8d20495e4b548f0ad5ec323aff7c",
            "9d49c106db914063a933e10236f7e0bd",
            "a8707bc83858420e957d8c42ebbe2e3b",
            "8e3d65dc8d6741e884b8985678895201",
            "78dafdba36a14b1c90d0ca38524b8f12",
            "5a933bfa70094f07a2c7efa2fbd952b3",
            "2958caea11244e22b60af5541727403f",
            "ea91635d846b4dbabe565b97aaf3aee3",
            "2bdcfa7743e643cc982d4ed61ae4c970",
            "107427356dd64cc0926d5ffc9aa80cc6",
            "416115c537e04a88895e6a2e8152b418",
            "578395115d2b458192e228de5357eb55",
            "9e26c5ee0f79499d9075008d460cf631",
            "895ae74a00004c25b0f016dec3ede2bb",
            "493b11052454443d829d980029800e4a",
            "ebff0a974f9d4efc836c5bfdec402de5",
            "6dca5c75547d4f78bcd5831c9f51b389",
            "35c2621e8ac6422eb201f7616fa94bdd",
            "4628912565dc409eb484ac59d8f0dffb",
            "d33eed0297ae48c387638a6478579d5e",
            "cb666b1ef0904b33b5c0248ee1b524c3",
            "51a0cc888f18415ba53d1eab62518e0d",
            "edfaed4a47954ce9991b0b894a0dbf7c",
            "f04524751ef44e3987420e2f870f0164",
            "4a2be97e4a6447be9f10057264b78a48",
            "f80fc677b9d2480fb0e5a824a3f11032",
            "31e5baddb7da4b6e93eae9b578bef37e",
            "8929fb1acf144d27ab7d592fb6e99f05",
            "161cab54a58647eb8a881a3f01fd3d94",
            "1f14f42251a2439bb2ab3f2a4b4800c9",
            "80cd948c7fde4f1da1346f256e868501",
            "333de635f4f44022a04b49a7a7155027",
            "fa3237618d024e27b2e2a9ff40064892",
            "9bdb7406f57c4deeb4f663c3c58a6b92",
            "7f6c72ec6af54e8885adc99619840352",
            "626d64057109443f8c8a1ff4627da87a",
            "06dbb95f22c04221826c10e160660624",
            "a586088bf63b4f9f80d898d3f0240885",
            "4093d36d2904437ba502385ee7f60268",
            "7c13b23ab81c4b98b5766bab459f52fe",
            "6f1b4c6d5ba74dcfb3bf517bb3e4ee4c",
            "8ca85c76ce0e4ad58df32be85df747f9",
            "23b3ebf7ee6c417cb294c62e9795e224",
            "938445b86b51426abdb663e2bf1d6136",
            "c8dbe2b025104941917877c61ec383cf",
            "89ac705d9394495dbb0ec47430e1e2a5",
            "1a243c89895c48df958b5b2b920874e7",
            "c833c4c58ff4475d9ec4781fa91720cd",
            "4c52e5dbd5cb447fa8a56a988054882f",
            "40dec5ec080e46d8ab6fd83860f2c8c2",
            "b198c5e0c4bc47c1b2964f0aefe33448",
            "7966662e54394544b1e2514913b3cb1c",
            "defe10fe7430422eb4efe4bdc50268f4",
            "215921d424234a2eb4a27d0e4dabcbf1",
            "4ed8f3049cf94987ad1cb10118d1f7be",
            "8f27d6d6ffa64dbab685b7b348662d81",
            "a027e8b1f49f4cd09d3dde6b54881df3",
            "3b0f69ec88ab4bfa97fe1969579344c5",
            "4340306ab97e420eb658cd0f5607ee9a",
            "025ed361d0bd4f789a90c5990044331d",
            "cb1c89c7910f4047934a538690a98491",
            "c1612d07c47a419e8bd7ed1fa0656a31",
            "2eb05b235bcd45d491f7a331382e70c8",
            "8f1474202abd4559bf95f16087b5a4c7",
            "84023bde639149c59f12b8aaea0cf534",
            "3eb8c144062f4fea85b4080197396c07",
            "907f5bcb83b64074991a3f38e3844267",
            "8bfd0118c89f4c13851fba980e250641",
            "b1b6a5ef8189411bac4fee8f38e34b4c",
            "da272f13f037431f9ae9fc63af7f3fce",
            "3946f007d18f494ba1b4ca2cc1f4dc95",
            "79ebd99dd79845899cf32cd85d5d5b59",
            "280b9d38456448e2b37518918b763367",
            "869a6d28f66e4aaebaa555f1706f78f5"
          ]
        },
        "id": "VZIcihMJF6zF",
        "outputId": "94fe3287-5a40-4af2-fd8e-8109558ec09f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f2bceea54ff4d8aa6283c4e6c41ddc1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00c46469b578408aaa0dbbe6fc660a20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb3f76fbe5e84b3697a91ac81c275c7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8426aeaefca047269d5203d205236b76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7acc65659a54c8d9f97d808215b1460",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2bdcfa7743e643cc982d4ed61ae4c970",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d33eed0297ae48c387638a6478579d5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80cd948c7fde4f1da1346f256e868501",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ca85c76ce0e4ad58df32be85df747f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "defe10fe7430422eb4efe4bdc50268f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f1474202abd4559bf95f16087b5a4c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "model_kwargs = {\"device\": \"cuda\"}\n",
        "embeddings = HuggingFaceEmbeddings(model_name= embedding_model_name, model_kwargs=model_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49SR46XRR-u8"
      },
      "source": [
        "# **Load Docs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZPaVdqffSBYN"
      },
      "outputs": [],
      "source": [
        "folder_path = 'contoh.pdf'\n",
        "loader = PyPDFLoader(folder_path)\n",
        "documents = loader.load()\n",
        "\n",
        "        # Split the document into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=150\n",
        ")\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjInx677JRIG",
        "outputId": "1fe780e8-59cd-415f-cdc7-78c42a2fe4d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'contoh.pdf', 'page': 0, 'page_label': '1'}, page_content='Embeddings  \\n& Vector Stores\\nAuthors: Anant Nawalgaria  \\nand Xiaoqi Ren'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 1, 'page_label': '2'}, page_content='Embeddings & Vector Stores\\n2\\nSeptember 2024\\nReviewers and Contributors\\nAntonio Gulli\\nGrace Mollison\\nRuiqi Guo\\nIftekhar Naim\\nJinhyuk Lee\\nAlan Li\\nPatricia Florissi\\nAndrew Brook\\nOmid Fatemieh\\nZhuyun Dai\\nLee Boonstra\\nPer Jacobsson\\nSiddhartha Reddy Jonnalagadda\\nXi Cheng\\nRaphael Hoffmann\\nCurators and Editors\\nAntonio Gulli\\nAnant Nawalgaria\\nGrace Mollison \\nTechnical Writer\\nJoey Haymaker\\nDesigner\\nMichael Lanning \\nAcknowledgements'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 2, 'page_label': '3'}, page_content='Introduction 5\\nWhy embeddings are important 6\\n Types of embeddings 9\\n  Text embeddings 9\\n   Word embeddings 11\\n   Document embeddings 15\\n    Shallow BoW models 16\\n    Deeper pretrained large language models 18\\n  Image & multimodal embeddings 22\\n  Structured data embeddings 24\\n   General structured data  24\\n   User/item structured data 25\\n  Graph embeddings 25\\n Training Embeddings 26\\nTable of contents'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 3, 'page_label': '4'}, page_content='Vector search 28\\n Important vector search algorithms 29\\n  Locality sensitive hashing & trees 30\\n  Hierarchical navigable small worlds  33\\n  ScaNN 34\\nVector databases  37\\n Operational considerations 39\\nApplications 40\\n Q & A with sources (retrieval augmented generation) 42\\nSummary 47\\nEndnotes 49'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 4, 'page_label': '5'}, page_content=\"Embeddings & Vector Stores\\n5\\nSeptember 2024\\nIntroduction\\nModern machine learning thrives on diverse data—images, text, audio, and more. This \\nwhitepaper explores the power of embeddings, which transform this heterogeneous data into \\na unified vector representation for seamless use in various applications.\\nWe'll guide you through:\\n• Understanding Embeddings: Why they are essential for handling multimodal data and \\ntheir diverse applications.\\n• Embedding Techniques: Methods for mapping different data types into a common \\nvector space.\\nThese low-dimensional numerical \\nrepresentations of real-world data \\nsignificantly helps efficient large-\\nscale data processing and storage \\nby acting as means of lossy \\ncompression of the original data.\"),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 5, 'page_label': '6'}, page_content='Embeddings & Vector Stores\\n6\\nSeptember 2024\\n• Efficient Management: Techniques for storing, retrieving, and searching vast collections \\nof embeddings.\\n• Vector Databases: Specialized systems for managing and querying embeddings, \\nincluding practical considerations for production deployment.\\n• Real-World Applications: Concrete examples of how embeddings and vector databases \\nare combined with large language models (LLMs) to solve real-world problems.\\nThroughout the whitepaper, code snippets provide hands-on illustrations of key concepts.\\nWhy embeddings are important\\nIn essence, embeddings are numerical representations of real-world data such as text, \\nspeech, image, or videos. They are expressed as low-dimensional vectors where the \\ngeometric distances of two vectors in the vector space is a projection of the relationships \\nbetween the two real-world objects that the vectors represent. In other words they help you'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 5, 'page_label': '6'}, page_content='between the two real-world objects that the vectors represent. In other words they help you \\nwith providing compact representations of data of different types, while simultaneously also \\nallowing you to compare two different data objects and tell how similar or different they are \\non a numerical scale: for example: The word ‘computer’ has a similar meaning to the picture \\nof a computer, as well as the word ’laptop’ but not to the word ‘car’. These low-dimensional \\nnumerical representations of real-world data significantly helps efficient large-scale data \\nprocessing and storage by acting as means of lossy compression of the original data while \\nretaining its important properties.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 6, 'page_label': '7'}, page_content='Embeddings & Vector Stores\\n7\\nSeptember 2024\\nOne of the key applications for embeddings is retrieval and recommendations, where the \\nresult is usually from a massive search space. For example, Google Search is a retrieval  with \\nthe  search space of the whole internet. Today’s retrieval and recommendation systems’ \\nsuccess depends on the following:\\n1. Precomputing the embeddings for billions items of the search space.\\n2. Mapping query embeddings to the same embedding space.\\n3. Efficient computing and retrieving of the nearest neighbors of the query embeddings in \\nthe search space.\\nEmbeddings also shine in the world of multimodality. Most applications work with large \\namounts of data of various modalities: text, speech, image, and videos to name a few. \\nBecause every entity or object is represented in its own unique format, it’s very difficult \\nto project these objects into the same vector space that is both compact and informative.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 6, 'page_label': '7'}, page_content='to project these objects into the same vector space that is both compact and informative. \\nIdeally, such a representation would capture as much of the original object’s characteristics \\nas possible. An embedding refers to the projected vector of an object from an input space to \\na relatively low-dimensional vector space. Each vector is a list of floating point numbers.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 7, 'page_label': '8'}, page_content='Embeddings & Vector Stores\\n8\\nSeptember 2024\\nFigure 1. Projecting objects/content into a joint vector space with semantic meaning\\nIdeally the embeddings are created so they place objects with similar semantic properties \\ncloser in the embedding space (a low-dimensional vector space where items can be \\nprojected). The embeddings can then be used as a condensed, meaningful input in \\ndownstream applications. For example, you can use them as features for ML models, \\nrecommender systems, search engines, and many more. So your data not only gets a \\ncompact numerical representation, but this representation also preserves the semantic \\nmeanings for a specific task or across a variety of tasks. The fact that these representations \\nare task-specific means you can generate different embeddings for the same object, \\noptimized for the task at hand.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 8, 'page_label': '9'}, page_content='Embeddings & Vector Stores\\n9\\nSeptember 2024\\nTypes of embeddings\\nEmbeddings aim to obtain a low dimensional representation of the original data while \\npreserving most of the ‘essential information’. The types of data an embedding represents \\ncan be of various different forms.  Below you’ll see some standard techniques used for \\ndifferent types of data, including text and image.\\nText embeddings\\nText embeddings are used extensively as part of natural language processing (NLP). They \\nare often used to embed the meaning of natural language in machine learning for processing \\nin various downstream applications such as text generation, classification, sentiment \\nanalysis, and more. These embeddings broadly fall into two categories: token/word and \\ndocument embeddings.\\nBefore diving deeper into these categories, it’s important to understand the entire lifecycle \\nof text: from its input by the user to its conversion to embeddings. \\nFigure 2. The process of turning text into embeddings'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 8, 'page_label': '9'}, page_content='of text: from its input by the user to its conversion to embeddings. \\nFigure 2. The process of turning text into embeddings\\nIt all starts with the input string which is split into smaller meaningful pieces called tokens.  \\nThis process is called tokenization. Commonly, these tokens are wordpieces, characters, \\nwords, numbers, and punctuations using one of the many existing tokenization techniques.1 \\nAfter the string is tokenized, each of these tokens is then assigned a unique integer value'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 9, 'page_label': '10'}, page_content='Embeddings & Vector Stores\\n10\\nSeptember 2024\\nusually in the the range: [0, cardinality of the total number of tokens in the corpus]. For \\nexample, for a 16 word vocabulary the IDs would range between 0-15. This value is also \\nreferred to as token ID. These tokens can be used to represent each string as a sparse \\nnumerical vector representation of documents used for downstream tasks directly, or after \\none-hot encoding. One-hot encoding is a binary representation of categorical values where \\nthe presence of a word is represented by 1, and its absence by 0. This ensures that the token \\nIDs are treated as categorical values as they are, but often results in a dense vector the size \\nof the vocabulary of the corpus. Snippet 1 and Figure 3 show an example of how this can be \\ndone using Tensorflow.\\n# Tokenize the input string data\\nfrom tensorflow.keras.preprocessing.text import Tokenizer\\ndata = [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 9, 'page_label': '10'}, page_content='data = [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\n# Filter the punctiations, tokenize the words and index them to integers  \\ntokenizer = Tokenizer(num_words=15, filters=\"!\"#$%&()*+,-./:;<=>?[\\\\\\\\]^_\\'{|}~\\\\t\\\\n\", lower=True, \\nsplit=\\' \\')\\ntokenizer.fit_on_texts(data)\\n# Translate each sentence into its word-level IDs, and then one-hot encode those IDs \\nID_sequences = tokenizer.texts_to_sequences(data)\\nbinary_sequences = tokenizer.sequences_to_matrix(ID_sequences)\\nprint(\"ID dictionary:\\\\n\", tokenizer.word_index)\\nprint(\"\\\\nID sequences:\\\\n\", ID_sequences)\\nprint(\"\\\\n One-hot encoded sequences:\\\\n\", binary_sequences )\\nSnippet 1. Tokenizing,  indexing and one-hot encoding strings'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 10, 'page_label': '11'}, page_content='Embeddings & Vector Stores\\n11\\nSeptember 2024\\nFigure 3. Output of Snippet 1\\nHowever, since these Integer IDs (or their corresponding one-hot encoded vectors) are \\nassigned randomly to words, they lack any inherent semantic meaning. This is where \\nembeddings are much more useful. Although it’s possible to embed character and sub-word \\nlevel tokens as well, let us look at word and document embeddings to understand some of \\nthe methods behind them.\\nWord embeddings\\nIn this section, you’ll see a few word embedding techniques and algorithms to both train \\nand use word embeddings. While there are many ML driven algorithms developed over \\ntime optimized for different objectives, the most common ones are GloVe,2 SWIVEL,3 and \\nWord2Vec.4 Word embeddings or sub-word embeddings can also be directly obtained from \\nhidden layers of language models. However, the embeddings will be different for the same \\nword in different contexts of the text. This section focuses on lightweight, context-free'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 10, 'page_label': '11'}, page_content='word in different contexts of the text. This section focuses on lightweight, context-free \\nword embedding and leaves the context-aware document embeddings for the document \\nembeddings section. Word embedding can be directly applied to downstream tasks like \\nnamed entity extraction and topic modeling.\\nWord2Vec is a family of model architectures that operates on the principle of “the semantic \\nmeaning of a word is defined by its neighbors”, or words that frequently appear close to each \\nother in the training corpus. This method can be both used to train your own embeddings'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 11, 'page_label': '12'}, page_content='Embeddings & Vector Stores\\n12\\nSeptember 2024\\nfrom large datasets or be quickly integrated through one of the readily available pre-trained \\nembeddings available online.5 The embeddings for each word - which are essentially fixed \\nlength vectors - are randomly initialized to kick off the process, resulting in a matrix of shape \\n(size_of_vocabulary, size_of_each_embedding). This matrix can be used as a lookup table \\nafter the training process is completed using one of the following methods (see Figure 4). \\n• The Continuous bag of words (CBOW) approach: Tries to predict the middle word, using \\nthe embeddings of the surrounding words as input. This method is agnostic to the order \\nof the surrounding words in the context. This approach is fast to train and is slightly more \\naccurate for frequent words.\\n• The skip-gram approach: The setup is inverse of that of CBOW, with the middle word \\nbeing used to predict the surrounding words within a certain range. This approach is'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 11, 'page_label': '12'}, page_content='being used to predict the surrounding words within a certain range. This approach is \\nslower to train but works well with small data and is more accurate for rare words.\\nFigure 4. Diagram explaining how CBOW and Skip-Gram methods work'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 12, 'page_label': '13'}, page_content='Embeddings & Vector Stores\\n13\\nSeptember 2024\\nThe Word2Vec algorithms can also be extended to the sub-word level, which has been the \\ninspiration for algorithms such as FastText.6 However, one of the major caveats of Word2Vec \\nis that although it accounts well for local statistics of words within a certain sliding window, it \\ndoes not capture the global statistics (words in the whole corpus). This shortcoming is what \\nmethods like the GloVe algorithm address.\\nGloVe is a word embedding technique that leverages both global and local statistics of words. \\nIt does this by first creating a co-occurrence matrix, which represents the relationships \\nbetween words. GloVe then uses a factorization technique to learn word representations \\nfrom the co-occurrence matrix. The resulting word representations are able to capture both \\nglobal and local information about words, and they are useful for a variety of NLP tasks.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 12, 'page_label': '13'}, page_content='global and local information about words, and they are useful for a variety of NLP tasks.\\nIn addition to GloVE, SWIVEL is another approach which leverages the co-occurrence \\nmatrix to learn word embeddings. SWIVEL stands for Skip-Window Vectors with Negative \\nSampling. Unlike GloVE, it uses local windows to learn the word vectors by taking into \\naccount the co-occurrence of words within a fixed window of its neighboring words. \\nFurthermore, SWIVEL also considers unobserved co-occurrences and handles it using a \\nspecial piecewise loss, boosting its performance with rare words. It is generally considered \\nonly slightly less accurate than GloVe on average, but is considerably faster to train. This is \\nbecause it leverages distributed training by subdividing the Embedding vectors into smaller \\nsub-matrices and executing matrix factorization in parallel on multiple machines. Snippet 2 \\nbelow demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 12, 'page_label': '13'}, page_content='below demonstrates loading pre-trained word embeddings for both Word2Vec and GloVe and \\nvisualizing them in a 2D space, and computing nearest neighbors.\\nWord embeddings can be directly used in some downstream tasks like Named Entity \\nRecognition (NER).'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 13, 'page_label': '14'}, page_content='Embeddings & Vector Stores\\n14\\nSeptember 2024\\nfrom gensim.models import Word2Vec \\nimport gensim.downloader as api\\nimport pprint\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import TSNE\\nimport numpy as np\\ndef tsne_plot(models, words, seed=23):\\n  \"Creates a TSNE models & plots for multiple word models for the given words\"\\n  plt.figure(figsize=(len(models)*30, len(models)*30))\\n  model_ix = 0\\n  for model in models:\\n    labels = []\\n    tokens = []\\n    for word in words:\\n      tokens.append(model[word])\\n      labels.append(word)\\n    tsne_model = TSNE(perplexity=40, n_components=2, init=\\'pca\\', n_iter=2500, random_state=seed) \\n    new_values = tsne_model.fit_transform(np.array(tokens))\\n    x = []\\n    y = []\\n    for value in new_values:\\n      x.append(value[0])\\n      y.append(value[1])\\n    model_ix +=1\\n    plt.subplot(10, 10, model_ix)\\n    for i in range(len(x)):\\n      plt.scatter(x[i],y[i])\\n      plt.annotate(labels[i],\\n            xy=(x[i], y[i]),\\n            xytext=(5, 2),'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 13, 'page_label': '14'}, page_content='for i in range(len(x)):\\n      plt.scatter(x[i],y[i])\\n      plt.annotate(labels[i],\\n            xy=(x[i], y[i]),\\n            xytext=(5, 2),\\n            textcoords=\\'offset points\\',\\n            ha=\\'right\\',\\n            va=\\'bottom\\')\\n  plt.tight_layout()\\n  plt.show()\\nv2w_model = api.load(\\'word2vec-google-news-300\\')\\nglove_model = api.load(\\'glove-twitter-25\\')\\nprint(\"words most similar to \\'computer\\' with word2vec and glove respectively:\")\\npprint.pprint( v2w_model.most_similar(\"computer\")[:3])\\npprint.pprint( glove_model.most_similar(\"computer\")[:3]) \\npprint.pprint(\"2d projection of some common words of both models\")\\nsample_common_words= list(set(v2w_model.index_to_key[100:10000]) \\n                        & set(glove_model.index_to_key[100:10000]))[:100]\\ntsne_plot([v2w_model, glove_model], sample_common_words)\\nSnippet 2. Loading and plotting GloVe and Word2Vec embeddings in 2D'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 14, 'page_label': '15'}, page_content='Embeddings & Vector Stores\\n15\\nSeptember 2024\\nFigure 5 Shows semantically similar words are clustered differently for the two algorithms\\nFigure 5. 2D visualization of pre-trained GloVe and Word2Vec word embeddings\\nDocument embeddings\\nEmbedding documents to low-dimensional dense embedding has attracted long-lasting \\ninterests since the 1980s. Document embeddings can be used in various applications, \\nincluding semantic search, topic discovery, classification, and clustering to embed \\nthe meaning of a series of words in paragraphs and documents and use it for various'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 15, 'page_label': '16'}, page_content='Embeddings & Vector Stores\\n16\\nSeptember 2024\\ndownstream applications. The evolution of the embeddings models can mainly be \\ncategorized into two stages: shallow Bag-of-words (BoW) models and deeper pretrained \\nlarge language models.\\nShallow BoW models\\nEarly document embedding works follow the bag-of-words (BoW) paradigm, assuming a \\ndocument is an unordered collection of words. These early works include latent semantic \\nanalysis (LSA)7 and latent dirichlet allocation (LDA).8 Latent semantic analysis (LSA) uses \\na co-occurrence matrix of words in documents and latent dirichlet allocation (LDA) uses a \\nbayesian network to model the document embeddings. Another famous bag-of-words family \\nof document embeddings is TF-IDF (term frequency-inverse document frequency) based \\nmodels, which are statistical models that use the word frequency to represent the document \\nembedding. TF-IDF-based models can either be a sparse embedding, which represents the'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 15, 'page_label': '16'}, page_content='embedding. TF-IDF-based models can either be a sparse embedding, which represents the \\nterm-level importance, or can be combined with word embeddings as a weighting factor to \\ngenerate a dense embedding for the documents. For example, BM25, a TF-IDF-based bag-\\nof-words model, is still a strong baseline in today’s retrieval benchmarks.9\\nHowever,  the bag-of-words paradigm also has two major weaknesses: both the word \\nordering and the semantic meanings are ignored. BoW models fail to capture the sequential \\nrelationships between words, which are crucial for understanding meaning and context. \\nInspired by Word2Vec, Doc2Vec10 was proposed in 2014 for generating document \\nembeddings using (shallow) neural networks. The Doc2Vec model adds an additional \\n‘paragraph’ embedding or, in other words, document embedding in the model of Word2Vec \\nas illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 15, 'page_label': '16'}, page_content='as illustrated in Figure 6. The paragraph embedding is concatenated or averaged with other \\nword embeddings to predict a random word in the paragraph. After training, for existing \\nparagraphs or documents, the learned embeddings can be directly used in downstream \\ntasks. For a new paragraph or document, extra inference steps need to be performed to \\ngenerate the paragraph or document embedding.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 16, 'page_label': '17'}, page_content='Embeddings & Vector Stores\\n17\\nSeptember 2024\\nFigure 6. Doc2vec CBOW model\\nSnippet 3 below shows how you can train your own doc2Vec models on a custom corpus:\\nfrom gensim.test.utils import common_texts\\nfrom gensim.models.Doc2Vec import Doc2Vec, TaggedDocument\\nfrom gensim.test.utils import get_tmpfile\\n#train model on a sequence of documents tagged with their IDs\\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(common_texts)]\\nmodel = Doc2Vec(documents, vector_size=8, window=3, min_count=1, workers=6)\\n# persist model to disk, and load it to infer on new documents\\nmodel_file = get_tmpfile(\"Doc2Vec_v1\")\\nmodel.save(model_file)\\nmodel = Doc2Vec.load(model_file)  \\nmodel.infer_vector([\"human\", \"interface\"])\\nSnippet 3. Self-supervised Training and inference using Doc2Vec on private corpus'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 17, 'page_label': '18'}, page_content='Embeddings & Vector Stores\\n18\\nSeptember 2024\\nThe success of applying neural networks in the embedding world inspired an increasing \\ninterest in using deep neural networks to generate embeddings. \\nDeeper pretrained large language models\\nMotivated by the development of deep neural networks, different embedding models and \\ntechniques were proposed, and the state-of-the-art models are refreshed frequently. Main \\nchanges of the models include: \\n1. Using more complex learning models, especially bi-directional deep neural network \\nmodels. \\n2. The use of massive pre-training on unlabeled text. \\n3. The use of a subword tokenizer. \\n4. Using fine-tuning for various downstream NLP tasks. \\nIn 2018, BERT11 - which stands for bidirectional encoder representations from transformers - \\nwas proposed with groundbreaking results on 11 NLP tasks. Transformer, the model paradigm \\nBERT based on, has become the mainstream model paradigm until today. Besides using a'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 17, 'page_label': '18'}, page_content='BERT based on, has become the mainstream model paradigm until today. Besides using a \\ntransformer as the model backbone, another key of BERT’s success is from pre-training with \\na massive unlabeled corpus. In pretraining, BERT utilized masked language model (MLM) as \\nthe pre-training objective. It did this by randomly masking some tokens of the input and using \\nthe masked token id as the prediction objective. This allows the model to utilize both the \\nright and left context to pretrain a deep bidirectional transformer. BERT also utilizes the next \\nsentence prediction task in pretraining. BERT outputs a contextualized embedding for every \\ntoken in the input. Typically, the embedding of the first token (a special token named [CLS]) is \\nused as the embedding for the whole input.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 18, 'page_label': '19'}, page_content='Embeddings & Vector Stores\\n19\\nSeptember 2024\\nFigure 7. The BERT architecture\\nBERT became the base model for multiple embedding models, including Sentence-\\nBERT,12 SimCSE,13 and E5.14 Meanwhile, the evolution of language models - especially large \\nlanguage models - never stops. T5 was proposed in 2019 with up to 11B parameters. PaLM \\nwas proposed in 2022 to push the large language model to a surprising 540B parameters. \\nModels like Gemini from Google, GPT models from OpenAI and Llama models from Meta are \\nalso evolving to newer generations at astonishing speed. Please refer to the whitepaper on \\nFoundational models for more information about some common LLMs.\\nNew embedding models based on large language models have been proposed. For example, \\nGTR and Sentence-T5 show better performance on retrieval and sentence similarity \\n(respectively) than BERT family models.\\nAnother approach to new embeddings models development is generating multi-vector'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 18, 'page_label': '19'}, page_content='(respectively) than BERT family models.\\nAnother approach to new embeddings models development is generating multi-vector \\nembeddings instead of a single vector to enhance the representational power of the models. \\nEmbedding models in this family include ColBERT15 and XTR.16'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 19, 'page_label': '20'}, page_content='Embeddings & Vector Stores\\n20\\nSeptember 2024\\nFigure 8. An illustration of the taxonomy diagram of the embedding models\\nAlthough the deep neural network models require a lot more data and compute time to train, \\nthey have much better performance compared to models using bag-of-words paradigms. \\nFor example, for the same word the embeddings would be different with different contexts. \\nSnippet 4 demonstrates how pre-trained document embedding models from Tensorflow-\\nhub17 (for example,Sentence t5)A and Vertex AIB can be used for training models with Keras \\nand TF datasets. Vertex Generative AI text embeddings can be used with the Vertex AI SDK, \\nLangchain, and Google’s BigQuery (Snippet 5) for embedding and advanced workflows.18\\nA. Note: not all models on https://tfhub.dev/ can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 19, 'page_label': '20'}, page_content='and the training datasets and consult the legal team before commercial usage. \\nB. Note: not all models on https://tfhub.dev/ can be commercially used. Please check the licenses of the models \\nand the training datasets and consult the legal team before commercial usage.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 20, 'page_label': '21'}, page_content='Embeddings & Vector Stores\\n21\\nSeptember 2024\\nimport vertexai\\nfrom vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\\n# Set the model name. For multilingual: use \"text-multilingual-embedding-002\"\\nMODEL_NAME = \"text-embedding-004\"\\n# Set the task_type, text and optional title as the model inputs.\\n# Available task_types are \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\", \\n# \"SEMANTIC_SIMILARITY\", # \"CLASSIFICATION\", and \"CLUSTERING\"\\nTASK_TYPE = \"RETRIEVAL_DOCUMENT\" \\nTITLE = \"Google\"\\nTEXT = \"Embed text.\"\\n# Use Vertex LLM text embeddings\\nembeddings_vx = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ndef LLM_embed(text):\\n    def embed_text(text):\\n        text_inp = TextEmbeddingInput(task_type=\"CLASSIFICATION\",   text=text.numpy())\\n        return np.array(embeddings_vx.get_embeddings([text_inp])[0].values)\\n output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\\n output.set_shape(( 768,))\\n return  output\\n# Embed strings using vertex LLMs'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 20, 'page_label': '21'}, page_content='output = tf.py_function(func=embed_text, inp=[text], Tout=tf.float32)\\n output.set_shape(( 768,))\\n return  output\\n# Embed strings using vertex LLMs\\nLLM_embeddings=train_data.map(lambda x,y: (LLM_embed(x), y))\\n# Embed strings in the tf.dataset using one of the tf hub models\\nembedding = \"https://tfhub.dev/google/sentence-t5/st5-base/1\"\\nhub_layer = hub.KerasLayer(embedding, input_shape=[],dtype=tf.string, trainable=True)\\n                          \\n# Train model \\nmodel = tf.keras.Sequential()\\nmodel.add(hub_layer) # omit this layer if using Vertex LLM embeddings\\nmodel.add(tf.keras.layers.Dense(16, activation=\\'relu\\'))\\nmodel.add(tf.keras.layers.Dense(1))\\nmodel.compile(optimizer=\\'adam\\',loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\\n  metrics=[\\'accuracy\\'])\\nhistory = model.fit(train_data.shuffle(100).batch(8))\\nSnippet 4. Creating & integrating text embeddings (Vertex, Tfhub) into keras text classification models'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 21, 'page_label': '22'}, page_content='Embeddings & Vector Stores\\n22\\nSeptember 2024\\nSELECT * FROM ML.GENERATE_TEXT_EMBEDDING(\\nMODEL my_project.my_company.llm_embedding_model,\\n(\\nSELECT review as content\\nFROM bigquery-public-data.imdb.reviews));\\nSnippet 5. Creating LLM based text embeddings in BigQuery for selected columns in a table\\nImage & multimodal embeddings\\nMuch like text, it’s also possible to create both image and multimodal embeddings. \\nUnimodal image embeddings can be derived in many ways: one of which is by training a \\nCNN or Vision Transformer model on a large scale image classification task (for example, \\nImagenet), and then using the penultimate layer as the image embedding. This layer has \\nlearnt some important discriminative feature maps for the training task. It contains a set of \\nfeature maps that are discriminative for the task at hand and can be extended to other tasks \\nas well. \\nTo obtain multimodal embeddings19 you take the individual unimodal text and image'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 21, 'page_label': '22'}, page_content='as well. \\nTo obtain multimodal embeddings19 you take the individual unimodal text and image \\nembeddings and their semantic relationships learnt via another training process. This \\ngives you a fixed size semantic representation in the same latent space. The below snippet \\n(Snippet 6) can be used to compute image and multimodal embeddings for images and text \\nand be used with a keras model directly (much like the text embedding example).'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 22, 'page_label': '23'}, page_content='Embeddings & Vector Stores\\n23\\nSeptember 2024\\nimport base64\\nimport tensorflow as tf\\nfrom google.cloud import aiplatform\\nfrom google.protobuf import struct_pb2\\n#fine-tunable layer for image embeddings which can be used for downstream keras model image_\\nembed=hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_ft1k_s/feature_\\nvector/2\",trainable=False) \\nclass EmbeddingPredictionClient:\\n  \"\"\"Wrapper around Prediction Service Client.\"\"\"\\n  def __init__(self, project : str,\\n    location : str = \"us-central1\",\\n    api_regional_endpoint: str = \"us-central1-aiplatform.googleapis.com\"):\\n    client_options = {\"api_endpoint\": api_regional_endpoint}\\n    self.client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)  \\n    self.location = location\\n    self.project = project\\n  def get_embedding(self, text : str = None, gs_image_path : str = None):\\n   #load the image from a bucket in google cloud storage'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 22, 'page_label': '23'}, page_content='def get_embedding(self, text : str = None, gs_image_path : str = None):\\n   #load the image from a bucket in google cloud storage\\n   with tf.io.gfile.GFile(gs_image_path, \"rb\") as f:\\n     image_bytes = f.read()\\n   if not text and not image_bytes:\\n    raise ValueError(\\'At least one of text or image_bytes must be specified.\\')\\n   #Initialize a protobuf data struct with the text and image inputs \\n   instance = struct_pb2.Struct()\\n    if text:\\n      instance.fields[\\'text\\'].string_value = text\\n      if image_bytes:\\n      encoded_content = base64.b64encode(image_bytes).decode(\"utf-8\")\\n      image_struct = instance.fields[\\'image\\'].struct_value\\n      image_struct.fields[\\'bytesBase64Encoded\\'].string_value = .string_value = encoded_content\\n     #Make predictions using the multimodal embedding model\\n     instances = [instance]\\n     endpoint = (f\"projects/{self.project}/locations/{self.location}\"\\n         \"/publishers/google/models/multimodalembedding@001\")'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 22, 'page_label': '23'}, page_content='endpoint = (f\"projects/{self.project}/locations/{self.location}\"\\n         \"/publishers/google/models/multimodalembedding@001\")\\n     response = self.client.predict(endpoint=endpoint, instances=instances)\\n     text_embedding = None\\n     if text:    \\n      text_emb_value = response.predictions[0][\\'textEmbedding\\']\\n      text_embedding = [v for v in text_emb_value]\\n     image_embedding = None\\n     if image_bytes:    \\n      image_emb_value = response.predictions[0][\\'imageEmbedding\\']\\n      image_embedding = [v for v in image_emb_value]\\nContinues next page...'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 23, 'page_label': '24'}, page_content='Embeddings & Vector Stores\\n24\\nSeptember 2024\\n return EmbeddingResponse (text_embedding=text_embedding, image_embedding=image_embedding)\\n#compute multimodal embeddings for text and images\\nclient.get_embedding(text=\"sample_test\", gs_image_path=\"gs://bucket_name../image_filename..\")\\nSnippet 6. Using Vertex API to create Multimodal embeddings Graph embeddings\\nStructured data embeddings\\nThere are two common ways to generate embeddings for structured data, one is more \\ngeneral while the other is more tailored for recommendation applications. \\nUnlike unstructured data, where a pre-trained embedding model is typically available, we \\nhave to create the embedding model for the structured data since it would be specific to \\na particular application.\\nGeneral structured data\\nGiven a general structured data table, we can create embedding for each row. This can be \\ndone by the ML models in the dimensionality reduction category, such as the PCA model.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 23, 'page_label': '24'}, page_content='done by the ML models in the dimensionality reduction category, such as the PCA model.\\nOne use case for these embeddings are for anomaly detection. For example, we can create \\nembeddings for anomaly detection using large data sets of labeled sensor information \\nthat identify anomalous occurrences.20 Another case use is to feed these embeddings \\nto downstream ML tasks such as classification. Compared to using the original high-\\ndimensional data, using embeddings to train a supervised model requires less data. This is \\nparticularly important in cases where training data is not sufficient.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 24, 'page_label': '25'}, page_content='Embeddings & Vector Stores\\n25\\nSeptember 2024\\nUser/item structured data\\nThe input is no longer a general structured data table as above. Instead, the input includes \\nthe user data, item/product data plus the data describing the interaction between user and \\nitem/product, such as rating score. \\nThis category is for recommendation purposes, as it maps two sets of data (user dataset, \\nitem/product/etc dataset) into the same embedding space. For recommender systems, we \\ncan create embeddings out of structured data that correlate to different entities such as \\nproducts, articles, etc. Again, we have to create our own embedding model. Sometimes this \\ncan be combined with unstructured embedding methods when images or text descriptions \\nare found.\\nGraph embeddings\\nGraph embeddings are another embedding technique that lets you represent not \\nonly information about a specific object but also its neighbors (namely, their graph'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 24, 'page_label': '25'}, page_content='only information about a specific object but also its neighbors (namely, their graph \\nrepresentation). Take an example of a social network where each person is a node, and the \\nconnections between people are defined as edges. Using graph embedding you can model \\neach node as an embedding, such that the embedding captures not only the semantic \\ninformation about the person itself, but also its relations and associations hence enriching \\nthe embedding. For example, if two nodes are connected by an edge, the vectors for those \\nnodes would be similar. You might then be able to predict who the person is most similar \\nto and recommend new connections. Graph embeddings can also be used for a variety of \\ntasks, including node classification, graph classification, link prediction, clustering, search, \\nrecommendation systems, and more. Popular algorithms21,22 for graph embedding include \\nDeepWalk, Node2vec, LINE, and GraphSAGE.23'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 25, 'page_label': '26'}, page_content='Embeddings & Vector Stores\\n26\\nSeptember 2024\\nTraining Embeddings\\nCurrent embedding models usually use dual encoder (two tower) architecture. For example, \\nfor the text embedding model used in question-answering, one tower is used to encode \\nthe queries and the other tower is used to encode the documents. For the image and text \\nembedding model, one tower is used to encode the images and the other tower is used \\nto encode the text. The model can have various sub architectures, depending on how the \\nmodel components are shared between the two towers. The following figure shows some \\narchitectures of the dual encoders.24 \\nFigure 9. Some architectures of dual encoders\\nThe loss used in embedding models training is usually a variation of contrastive loss, which \\ntakes a tuple of <inputs, positive targets, [optional] negative targets> as the inputs. Training \\nwith contrastive loss brings positive examples closer and negative examples far apart.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 25, 'page_label': '26'}, page_content='with contrastive loss brings positive examples closer and negative examples far apart.\\nSimilar to foundation model training, training of an embedding model from scratch usually \\nincludes two stages: pretraining (unsupervised learning) and fine tuning (supervised \\nlearning). Nowadays, the embedding models are usually directly initialized from foundation \\nmodels such as BERT, T5, GPT, Gemini, CoCa. You can use these base models to leverage the \\nmassive knowledge that has been learned from the large-scale pretraining of the foundation'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 26, 'page_label': '27'}, page_content='Embeddings & Vector Stores\\n27\\nSeptember 2024\\nmodels. The fine-tuning of the embedding models can have one or more phases. The fine-\\ntuning datasets can be created in various methods, including human labeling, synthetic \\ndataset generation, model distillation, and hard negative mining.\\nTo use embeddings for downstream tasks like classification or named entity recognition, \\nextra layers (for example, softmax classification layer) can be added on top of the embedding \\nmodels. The embedding model can either be frozen (especially when the training dataset is \\nsmall), trained from scratch, or fine-tuned together with the downstream tasks. \\nVertex AI provides the ability to customize the Vertex AI text embedding models.25 Users can \\nalso choose to fine-tune the models directly. See26 for an example of fine tuning the BERT \\nmodel using tensorflow model garden. You can also directly load the embedding models from'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 26, 'page_label': '27'}, page_content='model using tensorflow model garden. You can also directly load the embedding models from \\ntfhub and fine-tune on top of the model. Snippet 7 shows an example how to build a classifier \\nbased on tfhub models. \\n# Can switch the embedding to different embeddings from different modalities on # \\ntfhub. Here we use the BERT model as an example.\\ntfhub_link = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\"\\nclass Classifier(tf.keras.Model):\\n    def __init__(self, num_classes):\\n      super(Classifier, self).__init__(name=\"prediction\")\\n        self.encoder = hub.KerasLayer(tfhub_link, trainable=True)\\n      self.dropout = tf.keras.layers.Dropout(0.1)\\n      self.dense = tf.keras.layers.Dense(num_classes)\\n    def call(self, preprocessed_text):\\n      encoder_outputs = self.encoder(preprocessed_text)\\n      pooled_output = encoder_outputs[\"pooled_output\"]\\n   x = self.dropout(pooled_output)\\n   x = self.dense(x)\\n   return x\\nSnippet 7. Creating a Keras model using trainable tfhub layer'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 27, 'page_label': '28'}, page_content='Embeddings & Vector Stores\\n28\\nSeptember 2024\\nSo far you’ve seen the various types of embeddings, techniques and best practices to train \\nthem for various data modalities, and some of their applications. The next section discusses \\nhow to persist and search the embeddings that have been created in a fast and scalable way \\nfor production workloads.\\nVector search\\nFull-text keyword search has been the lynchpin of modern IT systems for years. Full-text \\nsearch engines and databases (relational and non-relational) often rely on explicit keyword \\nmatching. For example, if you search for ‘cappuccino’ the search engine or database returns \\nall documents that mention the exact query in the tags or text description. However, if the \\nkey word is misspelled or described with a differently worded text, a traditional keyword \\nsearch returns incorrect or no results. There are traditional approaches which are tolerant of'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 27, 'page_label': '28'}, page_content='search returns incorrect or no results. There are traditional approaches which are tolerant of \\nmisspellings and other typographical errors. However, they are still unable to find the results \\nhaving the closest underlying semantic meanings to the query. This is where vector search is \\nvery powerful: it uses the vector or embedded semantic representation of documents.\\nVector search lets you to go beyond searching for exact query literals and allows you to \\nsearch for the meaning across various data modalities. This provides you more nuanced \\nresults. After you have a function that can compute embeddings of various items,  you \\ncompute the embedding of the items of interest and store this embedding in a database. \\nYou then embed the incoming query in the same vector space as the items. Next, you have \\nto find the best matches to the query. This process is analogous to finding the most ‘similar’'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 27, 'page_label': '28'}, page_content='to find the best matches to the query. This process is analogous to finding the most ‘similar’ \\nmatches across the entire collection of searchable vectors: similarity between vectors can be \\ncomputed using a metric such as euclidean distance, cosine similarity, or dot product.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 28, 'page_label': '29'}, page_content='Embeddings & Vector Stores\\n29\\nSeptember 2024\\nFigure 10. Visualization of how different metrics compute vector similarity\\nEuclidean distance (i.e., L2 distance) is a geometric measure of the distance between two \\npoints in a vector space. This works well for lower dimensions. Cosine similarity is a measure \\nof the angle between two vectors. And inner/dot product, is the projection of one vector \\nonto another. They are equivalent when the vector norms are 1. This seems to work better \\nfor higher dimensional data. Vector databases store and help manage and operationalize the \\ncomplexity of vector search at scale, while also addressing the common database needs.\\nImportant vector search algorithms\\nThe most straightforward way to find the most similar match is to run a traditional linear \\nsearch by comparing the query vector with each document vector and return the one with \\nthe highest similarity. However, the runtime of this approach scales linearly (O(N)) with the'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 28, 'page_label': '29'}, page_content='the highest similarity. However, the runtime of this approach scales linearly (O(N)) with the \\namount of documents or items to search. This approach is unacceptably slow for most use \\ncases involving several millions of documents or more. Using approximate nearest neighbour'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 29, 'page_label': '30'}, page_content='Embeddings & Vector Stores\\n30\\nSeptember 2024\\n(ANN) search for that purpose is more practical.  ANN is a technique for finding the closest \\npoints to a given point in a dataset with a small margin of error - but with a tremendous boost \\nin performance. There are many approaches with varying trade-offs across scale, indexing \\ntime, performance, simplicity and more.27 They use one or more implementations of the \\nfollowing techniques: quantization, hashing, clustering and trees, among others. Some of the \\nmost popular approaches are discussed below.\\nLocality sensitive hashing & trees\\nLocality sensitive hashing (LSH) 28 is a technique for finding similar items in a large dataset. \\nIt does this by creating one or more hash functions that map similar items to the same hash \\nbucket with high probability. This means that you can quickly find all of the similar items to \\na given item by only looking at the candidate items in the same hash bucket (or adjacent'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 29, 'page_label': '30'}, page_content='a given item by only looking at the candidate items in the same hash bucket (or adjacent \\nbuckets) and do a linear search amongst those candidate pairs. This allows for significantly \\nfaster lookups within a specific radius. The number of hash functions/tables and buckets \\ndetermine the search recall/speed tradeoff, as well as the false positive / true positive one. \\nHaving too many hash functions might cause similar items to different buckets, while too few \\nmight result in too many items falsely being hashed to the same bucket and the number of \\nlinear searches to increase.\\nAnother intuitive way to think about LSH is grouping residences by their postal code or \\nneighborhood name. Then based on where someone chooses to move you look at the \\nresidences for only that neighborhood and find the closest match.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 30, 'page_label': '31'}, page_content='Embeddings & Vector Stores\\n31\\nSeptember 2024\\nFigure 11. Visualization of how LSH uses random hyperplanes to partition the vector space\\nTree-based algorithms work similarly. For example, the Kd-tree approach works by creating \\nthe decision boundaries by computing the median of the values of the first dimension, then \\nthat of the second dimension and so on. This approach is very much like a decision tree. \\nNaturally this can be ineffective if searchable vectors are high dimensional. In that case, the \\nBall-tree algorithm is better suited. It is similar in functionality, except instead of going by \\ndimension-wise medians it creates buckets based on the radial distance of the data points \\nfrom the center. Here is an example of the implementation of these three approaches:'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 31, 'page_label': '32'}, page_content='Embeddings & Vector Stores\\n32\\nSeptember 2024\\nfrom sklearn.neighbors import NearestNeighbors\\nfrom vertexai.language_models import TextEmbeddingModel\\nfrom lshashing import LSHRandom\\nimport numpy as np\\nmodel = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@004\")\\ntest_items= [\\n  \"The earth is spherical.\",\\n  \"The earth is a planet.\",\\n  \"I like to eat at a restaurant.\"]\\nquery = \"the shape of earth\"\\nembedded_test_items = np.array([embedding.values for embedding in model.get_embeddings(test_items)])\\nembedded_query = np.array(model.get_embeddings([query])[0].values)\\n#Naive brute force search\\nn_neighbors=2\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'brute\\').fit(embedded_test_items) \\nnaive_distances, naive_indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#algorithm- ball_tree due to high dimensional vectors or kd_tree otherwise\\nnbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=\\'ball_tree\\').fit(embedded_test_items)'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 31, 'page_label': '32'}, page_content=\"nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(embedded_test_items) \\ndistances, indices = nbrs.kneighbors(np.expand_dims(embedded_query, axis = 0))\\n#LSH\\nlsh_random_parallel = LSHRandom(embedded_test_items, 4, parallel = True)\\nlsh_random_parallel.knn_search(embedded_test_items, embedded_query, n_neighbors, 3, parallel = True)\\n#output for all 3 indices = [0, 1] , distances [0.66840428, 0.71048843] for the first 2 neighbours\\n#ANN retrieved the same ranking of items as brute force in a much scalable manner\\nSnippet 8. Using scikit-learn 29  and lshashing 30  for ANN with LSH, KD/Ball-tree and linear search\\nHashing and tree-based approaches can also be combined and extended upon to obtain \\nthe optimal tradeoff between recall and latency for search algorithms. FAISS with HNSW and \\nScaNN are good examples.\"),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 32, 'page_label': '33'}, page_content='Embeddings & Vector Stores\\n33\\nSeptember 2024\\nHierarchical navigable small worlds \\nFigure 12. Diagram showing how HNSW ‘zooms in’ to perform ANN\\nOne of the FAISS (Facebook AI similarity search) implementations leverages the concept \\nof hierarchical navigable small world (HNSW) 31 to perform vector similarity search in sub-\\nlinear (O(Logn)) runtime with a good degree of accuracy. A HNSW is a proximity graph with a \\nhierarchical structure where the graph links are spread across different layers. The top layer \\nhas the longest links and the bottom layer has the shortest ones. As shown in Figure 9, the \\nsearch starts at the topmost layer where the algorithm greedily traverses the graph to find \\nthe vertex most semantically similar to the query. Once the local minimum for that layer is \\nfound, it then switches to the graph for the closest vertex on the layer below. This process \\ncontinues iteratively until the local minimum for the lowest layer is found, with the algorithm'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 32, 'page_label': '33'}, page_content='continues iteratively until the local minimum for the lowest layer is found, with the algorithm \\nkeeping track of all the vertices traversed to return the K-nearest neighbors. This algorithm \\ncan be optionally augmented with quantization and vector indexing to boost speed and \\nmemory efficiency.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 33, 'page_label': '34'}, page_content='Embeddings & Vector Stores\\n34\\nSeptember 2024\\nimport faiss\\nM=32 #creating high degree graph:higher recall for larger index & searching time\\nd=768 # dimensions of the vectors/embeddings\\nindex = faiss.IndexHNSWFlat(d, M)\\nindex.add(embedded_test_items) #build the index using the embeddings in Snippet 9\\n#execute the ANN search\\nindex.search(np.expand_dims(embedded_query, axis=0), k=2)\\nSnippet 9. Indexing and executing ANN search with the FAISS library using HNSW\\nScaNN\\nGoogle developed the scalable approximate nearest neighbor (ScaNN)32,33 approach which is \\nused across a lot of its products and services. This includes being externally available to all \\ncustomers of Google Cloud through the Vertex AI Vector Search. Below is how ScaNN uses \\na variety of steps to perform efficient vector search, with each one of them having their own \\nsubset of parameters. \\nThe first step is the optional partitioning step during training: it uses one of the multiple'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 33, 'page_label': '34'}, page_content='subset of parameters. \\nThe first step is the optional partitioning step during training: it uses one of the multiple \\nalgorithms available to partition the vector store into logical partitions/clusters where \\nthe semantically related are grouped together. The partitioning step is optional for small \\ndatasets. However, for larger datasets with >100k embedding vectors, the partitioning step \\nis crucial since by pruning the search space it cuts down the search space by magnitudes \\ntherefore significantly speeds up the query. The space pruning is configured through the \\nnumber of partitions and the number of partitions to search. A larger number leads to better \\nrecall but larger partition creation time. A good heuristic is to set the number of partitions to \\nbe the square root of the number of vectors.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 34, 'page_label': '35'}, page_content='Embeddings & Vector Stores\\n35\\nSeptember 2024\\nFigure 13. Search space partitioning & pruning(left) & Approximate scoring (right)\\nAt query time ScaNN uses the user-specified distance measure to select the specified \\nnumber of top partitions (a value specified by the user), and then executes the scoring \\nstep next. In this step ScaNN compares the query with all the points in the top partitions \\nand selects the top K’. This distance computation can be configured as exact distance or \\napproximate distance. The approximate distance computation leverages either standard \\nproduct quantization or anisotropic quantization techniques, the latter of which is a specific \\nmethod employed by ScaNN which gives the better speed and accuracy tradeoffs.\\nFinally, as a last step the user can optionally choose to rescore the user specified top K \\nnumber of results more accurately. This results in an industry leading speed/accuracy'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 34, 'page_label': '35'}, page_content='number of results more accurately. This results in an industry leading speed/accuracy \\ntradeoff ScaNN is known for as can be inferred from Figure 14. Snippet 10 shows a \\ncode example.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 35, 'page_label': '36'}, page_content='Embeddings & Vector Stores\\n36\\nSeptember 2024\\nFigure 14. Accuracy/speed tradeoffs for various SOTA ANN search algorithms\\nimport tensorflow as tf\\nimport tensorflow_recommenders as tfrs\\nfrom vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\\n# Embed documents & query(from snip 9.) and convert them to tensors and tf.datasets\\nembedded_query = tf.constant((LM_embed(query, \"RETRIEVAL_QUERY\")))\\nembedded_docs = [LM_embed(doc, \"RETIREVAL_DOCUMENT\") for doc in searchable_docs]\\nembedded_docs = tf.data.Dataset.from_tensor_slices(embedded_docs).enumerate().batch(1)\\n# Build index from tensorflow dataset and execute ANN search based on dot product metric\\nscann = tfrs.layers.factorized_top_k.ScaNN( \\n  distance_measure= \\'dot_product\\',\\n  num_leaves = 4, #increase for higher number of partitions / latency for increased recall\\n  num_leaves_to_search= 2) # increase for higher recall but increased latency\\nscann = scann.index_from_dataset(embedded_docs)\\nscann(embedded_query, k=2)'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 36, 'page_label': '37'}, page_content='Embeddings & Vector Stores\\n37\\nSeptember 2024\\nSnippet 10. Using Tensorflow Recommenders 34  to perform ANN search using the ScaNN algorithm\\nIn this whitepaper we have seen both State-of-the-Art SOTA and traditional ANN search \\nalgorithms: ScaNN, FAISS , LSH, KD-Tree, and Ball-tree, and examined the  great speed/\\naccuracy tradeoffs that they provide. However, to use these algorithms they need to \\nbe deployed in a scalable, secure and production-ready manner. For that we need \\nvector databases.\\nVector databases \\nVector embeddings embody semantic meanings of data, while vector search algorithms \\nprovide a means for efficiently querying them. Historically traditional databases lacked the \\nmeans to combine semantic meaning and efficient querying  in a way that the most relevant \\nembeddings can be both stored, queried, and retrieved in a secure, scalable, and flexible \\nmanner for complex analysis and real-time enterprise grade applications. This is what'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 36, 'page_label': '37'}, page_content='manner for complex analysis and real-time enterprise grade applications. This is what \\ngave rise to vector databases, which are built ground-up to manage these embeddings for \\nproduction scenarios. Due to the recent popularity of Generative AI, an increasing number \\nof traditional databases are starting to incorporate supporting vector search functionality \\nas well in addition to traditional search (‘hybrid search’) functionalities. Let’s look at the \\nworkflow for a simple Vector Database, with hybrid search capabilities.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 37, 'page_label': '38'}, page_content='Embeddings & Vector Stores\\n38\\nSeptember 2024\\nFigure 15. Populating and querying vector databases\\nEach vector database differs in its implementation, but the general flow is shown in Figure 15:\\n1. An appropriate trained embedding model is used to embed the relevant data points as \\nvectors with fixed dimensions. \\n2. The vectors are then augmented with appropriate metadata and complementary \\ninformation (such as tags) and indexed using the specified algorithm for efficient search.\\n3. An incoming query gets embedded with the same model, and used to query and return  \\nspecific amounts of the most semantically similar items and their associated unembedded \\ncontent/metadata. Some databases might provide caching and pre-filtering (based on \\ntags) and post-filtering capabilities (reranking using another more accurate model) to \\nfurther enhance the query speed and performance.\\nThere are quite a few vector databases available today, each tailored to different business'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 37, 'page_label': '38'}, page_content='further enhance the query speed and performance.\\nThere are quite a few vector databases available today, each tailored to different business \\nneeds and considerations. A few good examples of commercially managed vector databases \\ninclude Google Cloud’s Vertex Vector Search,35 Google Cloud’s AlloyDB & Cloud SQL \\nPostgres ElasticSearch,36 and Pinecone37 to name a few. Vertex AI Vector Search is a vector \\ndatabase built by Google that uses the ScaNN algorithm for fast vector search, while still \\nmaintaining all the security and access guarantees of Google Cloud. AlloyDB & Cloud SQL \\nPostgres supports vector search through the OSS pgvector38 extension, which allows for \\nSQL queries to combine ANN search with traditional predicates and the usual transactional \\nsemantics for ANN search index. AlloyDB also has a ScaNN index extension that is a native \\nimplementation of ScaNN and is pgvector-compatible. Similarly, many of the other traditional'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 37, 'page_label': '38'}, page_content='implementation of ScaNN and is pgvector-compatible. Similarly, many of the other traditional \\ndatabases have also started to add plugins to enable vector search. Pinecone and Weaviate \\nleverage HNSW for their fast vector search in addition to the ability to filter data using'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 38, 'page_label': '39'}, page_content='Embeddings & Vector Stores\\n39\\nSeptember 2024\\ntraditional search. Amongst their open source peers: Weaviate39 and ChromaDB40 provide a \\nfull suite of functionality upon deployment and can be tested in memory as well during the \\nprototyping phase.\\nOperational considerations\\nVector Databases are critical to managing the majority of technical challenges that arise \\nwith storing and querying embeddings at scale. Some of these challenges are specific to the \\nnature of vector stores, while others overlap with that of traditional databases. These include \\nhorizontal and vertical scalability, availability, data consistency, real time updates, backups, \\naccess control, compliance, and much more. However, there are also many more challenges \\nand considerations you need to take into account while using embedding and vector stores.\\nFirstly, embeddings, unlike traditional content, can mutate over time. This means that the'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 38, 'page_label': '39'}, page_content='Firstly, embeddings, unlike traditional content, can mutate over time. This means that the \\nsame text, image, video or other content could and should be embedded using different \\nembedding models to optimize for the performance of the downstream applications. This is \\nespecially true for embeddings of supervised models after the model is retrained to account \\nfor various drifts or changing objectives. Similarly, the same applies to unsupervised models \\nwhen they are updated to a newer model. However, frequently updating the embeddings \\n- especially those trained on large amounts of data - can be prohibitively expensive. \\nConsequently, a balance needs to be struck. This necessitates a well-defined automated \\nprocess to store, manage, and possibly purge embeddings from the vector databases taking \\nthe budget into consideration.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 39, 'page_label': '40'}, page_content='Embeddings & Vector Stores\\n40\\nSeptember 2024\\nSecondly, while embeddings are great at representing semantic information, sometimes they \\ncan be suboptimal at representing literal or syntactic information. This is especially true for \\ndomain-specific words or IDs. These values are potentially missing or underrepresented \\nin the data the embeddings models were trained on. For example, if a user enters a query \\nthat contains the ID of a specific number along with a lot of text, the model might find \\nsemantically similar neighbors which match the meaning of the text closely, but not the ID, \\nwhich is the most important component in this context. You can overcome this challenge by \\nusing a combination of full-text search to pre-filter or post-filter the search space before \\npassing it onto the semantic search module.\\nAnother important point to consider is that depending on the nature of the workload in which'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 39, 'page_label': '40'}, page_content='passing it onto the semantic search module.\\nAnother important point to consider is that depending on the nature of the workload in which \\nthe semantic query occurs, it might be worth relying on different vector databases. For \\nexample, for OLTP workloads that require frequent reads/write operations, an operational \\ndatabase like Postgres or CloudSQL is the best choice. For large-scale OLAP analytical \\nworkloads and batch use cases, using Bigquery’s vector search is preferable.\\nIn conclusion, a variety of factors need to be considered when choosing a vector database. \\nThese factors include size and type of your dataset (some are good at sparse and others \\ndense), business needs, the nature of the workload,  budget, security, privacy guarantees, \\nthe needs for semantic and syntactic search as well as the database systems that are already \\nin use. In this section we have seen the various ANN search approaches as well the need and'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 39, 'page_label': '40'}, page_content='in use. In this section we have seen the various ANN search approaches as well the need and \\nbenefits of vector databases. The next section demonstrates an example of using a Vector AI \\nVector Search for semantic search.\\nApplications\\nEmbeddings models are one of the fundamental machine learning models that power a \\nvariety of applications. We summarize some popular applications in the following table.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 40, 'page_label': '41'}, page_content='Embeddings & Vector Stores\\n41\\nSeptember 2024\\nTask Description\\nRetrieval\\nGiven a query and a set of objects (for example, documents, images, \\nand videos), retrieve the most relevant objects. Based on the definition \\nof relevant objects, the subtasks include question answering and \\nrecommendations.\\nSemantic text similarity\\nDetermine whether two sentences have the same semantic meaning. \\nThe subtasks include: paraphrasing, duplicate detection, and bitext \\nmining.\\nClassification\\nClassify objects into possible categories. Based on the number of labels, \\nthe subtasks include binary classification, multi-class classification, and \\nmultilabel classifications.\\nClustering Cluster objects together.\\nReranking Rerank a set of objects based on a certain query.\\n \\nEmbeddings together with vector stores providing ANN can be powerful tools which can be \\nused for a variety of applications. These include Retrieval augmented Generation for LLMs,'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 40, 'page_label': '41'}, page_content='used for a variety of applications. These include Retrieval augmented Generation for LLMs, \\nSearch, Recommendation Systems, Anomaly detection, few shot- classification and much \\nmore. \\nFor ranking problems like search and recommendations, embeddings are normally used \\nat the first stage of the process. They retrieve the potentially good candidates that are \\nsemantically similar and consequently improve the relevance of search results. Since the \\namount of information to sort through can be quite large (in some cases even millions or \\nbillions) ANN techniques like ScaNN greatly aids in scalably narrowing the search space. \\nLet’s look at an application which combines both LLMs and RAG to help answer questions.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 41, 'page_label': '42'}, page_content='Embeddings & Vector Stores\\n42\\nSeptember 2024\\nQ & A with sources (retrieval augmented generation)\\nRetrieval augmented generation (RAG) for Q&A is a technique that combines the best of both \\nworlds from retrieval and generation. It first retrieves relevant documents from a knowledge \\nbase and then uses prompt expansion to generate an answer from those documents. Prompt \\nexpansion is a technique that when combined with database search can be very powerful. \\nWith prompt expansion the model retrieves relevant information from the database (mostly \\nusing a combination of semantic search and business rules), and augments the original \\nprompt with it. The model uses this augmented prompt to generate much more interesting, \\nfactual, and informative content than with retrieval or generation alone.\\nRAGs can help with a common problem with LLMs: their tendency to ‘hallucinate’ and \\ngenerate factually incorrect but plausible sounding responses. Although RAG can reduce'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 41, 'page_label': '42'}, page_content='generate factually incorrect but plausible sounding responses. Although RAG can reduce \\nhallucinations, it does not completely eliminate them. What can help mitigate this problem \\nfurther is to also return the sources from the retrieval and do a quick coherence check either \\nby a human or an LLM. This ensures the LLM response is consistent with the semantically \\nrelevant sources. Let’s look at an example (Snippet 11 and 12) of RAG with sources, which can \\nbe scalably implemented using Vertex AI LLM text embeddings and Vertex AI Vector Search \\nin conjunction with libraries like langchain.41 We start with the initial setup in Snippet 11.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 42, 'page_label': '43'}, page_content='Embeddings & Vector Stores\\n43\\nSeptember 2024\\n# Before you start run this command:\\n# pip install --upgrade --user --quiet google-cloud-aiplatform langchain_google_vertexai\\n# after running pip install make sure you restart your kernel\\n# TODO : Set values as per your requirements\\n# Project and Storage Constants\\nPROJECT_ID = \"<my_project_id>\"\\nREGION = \"<my_region>\"\\nBUCKET = \"<my_gcs_bucket>\"\\nBUCKET_URI = f\"gs://{BUCKET}\"\\n# The number of dimensions for the textembedding-gecko@004 is 768\\n# If other embedder is used, the dimensions would probably need to change.\\nDIMENSIONS = 768\\n# Index Constants\\nDISPLAY_NAME = \"<my_matching_engine_index_id>\"\\nDEPLOYED_INDEX_ID = \"yourname01\" # you set this. Start with a letter.\\nfrom google.cloud import aiplatform\\nfrom langchain_google_vertexai import VertexAIEmbeddings\\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\\nembedding_model = VertexAIEmbeddings (model_name=\"textembedding-gecko@003\")'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 42, 'page_label': '43'}, page_content='embedding_model = VertexAIEmbeddings (model_name=\"textembedding-gecko@003\")\\n# NOTE : This operation can take upto 30 seconds\\nmy_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\\n    display_name=DISPLAY_NAME,\\n    dimensions=DIMENSIONS,\\n    approximate_neighbors_count=150,\\n    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\\n    index_update_method=\"STREAM_UPDATE\",  # allowed values BATCH_UPDATE , STREAM_UPDATE\\n)\\n# Create an endpoint\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n    display_name=f\"{DISPLAY_NAME}-endpoint\", public_endpoint_enabled=True\\n)\\n# NOTE : This operation can take upto 20 minutes\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n    index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n)\\nContinues next page...'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 43, 'page_label': '44'}, page_content='Embeddings & Vector Stores\\n44\\nSeptember 2024\\nSnippet 11. Setting up the network and environment\\nmy_index_endpoint.deployed_indexes\\n# TODO : replace 1234567890123456789 with your acutial index ID\\nmy_index = aiplatform.MatchingEngineIndex(\"1234567890123456789\")\\n# TODO : replace 1234567890123456789 with your acutial endpoint ID\\n# Be aware that the Index ID differs from the endpoint ID\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint(\"1234567890123456789\")\\nfrom langchain_google_vertexai import (\\n    VectorSearchVectorStore,\\n    VectorSearchVectorStoreDatastore,\\n)\\n# Input texts\\ntexts = [\\n    \"The cat sat on\",\\n    \"the mat.\",\\n    \"I like to\",\\n    \"eat pizza for\",\\n    \"dinner.\",\\n    \"The sun sets\",\\n    \"in the west.\",\\n]\\n# Create a Vector Store\\nvector_store = VectorSearchVectorStore.from_components(\\n    project_id=PROJECT_ID,\\n    region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 43, 'page_label': '44'}, page_content='region=REGION,\\n    gcs_bucket_name=BUCKET,\\n    index_id=my_index.name,\\n    endpoint_id=my_index_endpoint.name,\\n    embedding=embedding_model,\\n    stream_update=True,\\n)\\n# Add vectors and mapped text chunks to your vectore store\\nvector_store.add_texts(texts=texts)\\n# Initialize the vectore_store as retriever\\nretriever = vector_store.as_retriever()\\n# perform simple similarity search on retriever\\nretriever.invoke(\"What are my options in breathable fabric?\")'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 44, 'page_label': '45'}, page_content='Embeddings & Vector Stores\\n45\\nSeptember 2024\\nThen we setup and initialize the Vector AI Vector Searc engine ANN index using Vertex text \\nembeddings and then use the Vertex LLMs to do prompt expansion using semantic search. \\nThis both grounds the LLMs in factuality and provides sources as well (Snippet 13).\\n# Create dummy embeddings to initialize the vector store\\nembeddings_vx = VertexAIEmbeddings()\\ninitial_config = {\\n  \"id\": str(uuid.uuid4()),\\n  \"embedding\": [float(x) for x in list(embeddings_vx.embed_documents(test_items)[0])], \\n}\\nwith open(\"data.json\", \"w\") as f:\\n  json.dump(initial_config, f)\\n#magic command to be run on terminal or jupyter notebooks\\n!gsutil cp data.json {EMBEDDING_DIR}/file.json\\n# Create dummy embeddings to initialize the vector store\\naiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)\\nmy_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\\n  display_name=DISPLAY_NAME,\\n  contents_delta_uri=EMBEDDING_DIR,\\n  dimensions=DIMENSIONS,'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 44, 'page_label': '45'}, page_content='display_name=DISPLAY_NAME,\\n  contents_delta_uri=EMBEDDING_DIR,\\n  dimensions=DIMENSIONS,\\n  leafNodeEmbeddingCount=1000,\\n  fractionLeafNodesToSearch=0.1,\\n  approximate_neighbors_count=2,\\n  distance_measure_type=\"DOT_PRODUCT_DISTANCE\")\\nmy_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\\n  display_name=f\"{DISPLAY_NAME}-endpoint\",\\n  network=VPC_NETWORK_FULL)\\nmy_index_endpoint = my_index_endpoint.deploy_index(\\n  index=my_index, deployed_index_id=DEPLOYED_INDEX_ID\\n) \\n#initialize Langchain retriever and add text embeddings to index\\ntexts = [\\n \"The earth is spherical.\",\\n \"The earth is a planet.\",\\n \"I like to eat at a restaurant.\",\\n]\\nvector_store = MatchingEngine.from_components(\\n  project_id=PROJECT_ID,\\n  region=REGION,\\nContinues next page...'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 45, 'page_label': '46'}, page_content='Embeddings & Vector Stores\\n46\\nSeptember 2024\\n  gcs_bucket_name=BUCKET_URI,\\n  index_id=my_index.name,\\n  endpoint_id=my_index_endpoint.name,\\n  embedding=embeddings_vx\\n)\\nvector_store.add_texts(texts=texts)\\nretriever=vector_store.as_retriever(search_kwargs={\\'k\\':1 })\\n#Create Retrieval augmented few-shot prompts to provide context to ground LLMs\\nprompt_template=\"\"\"You are David, an AI knowledge bot. \\nAnswer the questions using the facts provided. Use the following pieces of context to answer \\nthe users question\\nIf you don\\'t know the answer, just say that \"I don\\'t know\", don\\'t try to make up an answer.\\n{summaries}\"\"\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(prompt_template),\\n    HumanMessagePromptTemplate.from_template(\"{question}\")\\n]\\nprompt = ChatPromptTemplate.from_messages(messages)\\nchain_type_kwargs = {\"prompt\": prompt}\\nllm = VertexAI() \\n#build your chain for RAG+C\\nchain= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 45, 'page_label': '46'}, page_content='chain_type_kwargs = {\"prompt\": prompt}\\nllm = VertexAI() \\n#build your chain for RAG+C\\nchain= RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \\nretriever=retriever, return_source_documents=True)\\n#print your results with Markup language\\ndef print_result(result):\\n  output_text = f\"\"\"### Question: \\n  {query}\\n  ### Answer: \\n  {result[\\'result\\']}\\n  ### Source: \\n  {\\' \\'.join(list(set([doc.page_content for doc in result[\\'source_documents\\']])))}\\n  \"\"\"\\n  return(output_text)\\nquery = \"What shape is the planet where humans live?\"\\nresult = chain(query)\\ndisplay(Markdown(print_result(result)))\\nSnippet 12. Build/deploy ANN Index for Vertex Matching engine and use RAG with LLM prompts to generate \\ngrounded results/sources'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 46, 'page_label': '47'}, page_content='Embeddings & Vector Stores\\n47\\nSeptember 2024\\nFigure 16. Model responses along with sources demonstrating the LLM being grounded in the database\\nAs we can infer from Figure 16, the output not only grounds LLM in the semantically similar \\nresults retrieved from the database (hence refusing to answer when context cannot be found \\nin the database). This not only significantly reduces hallucination, but also provides sources \\nfor verification, either human or using another LLM.\\nSummary\\nIn this whitepaper we have discussed various methods to create, manage, store, and retrieve \\nembeddings of various data modalities effectively in the context of production-grade \\napplications. Creating, maintaining and using embeddings for downstream applications can \\nbe a complex task that involves several roles in the organization. However, by thoroughly \\noperationalizing and automating its usage, you can safely leverage the incredible benefits'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 46, 'page_label': '47'}, page_content='operationalizing and automating its usage, you can safely leverage the incredible benefits \\nthey offer across some of the most important applications. Some key takeaways from this \\nwhitepaper include:\\n1. Choose your embedding model wisely for your data and use case. Ensure the data used in \\ninference is consistent with the data used in training. The distribution shift from training to \\ninference can come from various areas, including domain distribution shift or downstream'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 47, 'page_label': '48'}, page_content='Embeddings & Vector Stores\\n48\\nSeptember 2024\\ntask distribution shift. If no existing embedding models fit the current inference data \\ndistribution, fine-tuning the existing model can significantly help on the performance. \\nAnother tradeoff comes from the model size. The large deep neural network (large \\nmultimodal models) based models usually have better performance but can come with a \\ncost of longer serving latency. Using Cloud-based embedding services can conquer the \\nabove issue by providing both high-quality and low-latency embedding service. For most \\nbusiness applications using a pre-trained embedding model provides a good baseline, \\nwhich can be further fine-tuned or integrated in downstream models. In case the data has \\nan inherent graph structure, graph embeddings can provide superior performance.\\n2. Once your embedding strategy is defined, it’s important to make the choice of the \\nappropriate vector database that suits your budget and business needs. It might seem'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 47, 'page_label': '48'}, page_content='appropriate vector database that suits your budget and business needs. It might seem \\nquicker to prototype with available open source alternatives, but opting for a more secure, \\nscalable, and battle-tested managed vector database is certain to be better off in the long \\nterm. There are various open source alternatives using one of the many powerful ANN \\nvector search algorithms, but ScaNN and HNSW have proven to provide some of the best \\naccuracy and performance trade offs in that order.\\n3. Embeddings combined with an appropriate ANN powered vector database is an \\nincredibly powerful tool and can be leveraged for various applications, including \\nSearch, Recommendation systems, and Retrieval augment generation for LLMs. This \\napproach can mitigate the hallucination problem and bolster verifiability and trust of \\nLLM-based systems.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 48, 'page_label': '49'}, page_content=\"Embeddings & Vector Stores\\n49\\nSeptember 2024\\nEndnotes\\n1. Rai, A., 2020, Study of various methods for tokenization. In Advances in Natural Language Processing. \\nAvailable at: https://doi.org/10.1007/978-981-15-6198-6_18\\n2. Pennington, J., Socher, R. & Manning, C., 2014, GloVe: Global Vectors for Word Representation. [online] \\nAvailable at: https://nlp.stanford.edu/pubs/glove.pdf .\\n3. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V. & Hinton, G., 2016, Swivel: Improving embeddings \\nby noticing what's missing. ArXiv, abs/1602.02215. Available at: https://arxiv.org/abs/1602.02215 .\\n4. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. & Dean, J., 2013, Efficient estimation of word representations \\nin vector space. ArXiv, abs/1301.3781. Available at: https://arxiv.org/pdf/1301.3781.pdf .\\n5. Rehurek, R., 2021, Gensim: open source python library for word and document embeddings. Available \\nat: https://radimrehurek.com/gensim/intro.html .\"),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 48, 'page_label': '49'}, page_content='at: https://radimrehurek.com/gensim/intro.html .\\n6. Bojanowski, P., Grave, E., Joulin, A. & Mikolov, T., 2016, Enriching word vectors with subword information. \\nArXiv, abs/1607.04606. Available at: https://arxiv.org/abs/1607.04606.\\n7. Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., & Harshman, R., 1990, Indexing by latent \\nsemantic analysis. Journal of the American Society for Information Science, 41(6), pp. 391-407.\\n8. Blei, D. M., Ng, A. Y., & Jordan, M. I., 2001, Latent Dirichlet allocation. In T. G. Dietterich, S. Becker, & Z. \\nGhahramani (Eds.), Advances in Neural Information Processing Systems 14. MIT Press, pp. 601-608. Available \\nat: https://proceedings.neurips.cc/paper/2001/hash/296472c9542ad4d4788d543508116cbc-Abstract.html .\\n9. Muennighoff, N., Tazi, N., Magne, L., & Reimers, N., 2022, Mteb: Massive text embedding benchmark. ArXiv, \\nabs/2210.07316. Available at: https://arxiv.org/abs/2210.07316 .'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 48, 'page_label': '49'}, page_content='abs/2210.07316. Available at: https://arxiv.org/abs/2210.07316 .\\n10. Le, Q. V., Mikolov, T., 2014, Distributed representations of sentences and documents. ArXiv, abs/1405.4053. \\nAvailable at: https://arxiv.org/abs/1405.4053 .\\n11. Devlin, J., Chang, M. W., Lee, K., & Toutanova, K., 2019, BERT: Pre-training deep Bidirectional Transformers \\nfor Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the \\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), \\npp. 4171-4186. Available at: https://www.aclweb.org/anthology/N19-1423/ .\\n12. Reimers, N. & Gurevych, I., 2020, Making monolingual sentence embeddings multilingual using knowledge \\ndistillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing \\n(EMNLP), pp. 254-265. Available at: https://www.aclweb.org/anthology/2020.emnlp-main.21/ .'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 49, 'page_label': '50'}, page_content='Embeddings & Vector Stores\\n50\\nSeptember 2024\\n13. Gao, T., Yao, X. & Chen, D., 2021, Simcse: Simple contrastive learning of sentence embeddings. ArXiv, \\nabs/2104.08821. Available at: https://arxiv.org/abs/2104.08821 .\\n14. Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R. & Wei, F., 2022, Text embeddings by \\nweakly supervised contrastive pre-training. ArXiv. Available at: https://arxiv.org/abs/2201.01279 .\\n15. Khattab, O. & Zaharia, M., 2020, colBERT: Efficient and effective passage search via contextualized late \\ninteraction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and \\nDevelopment in Information Retrieval, pp. 39-48. Available at: https://dl.acm.org/doi/10.1145/3397271.3401025 .\\n16. Lee, J., Dai, Z., Duddu, S. M. K., Lei, T., Naim, I., Chang, M. W. & Zhao, V. Y., 2023, Rethinking the role of token \\nretrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982 .'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 49, 'page_label': '50'}, page_content='retrieval in multi-vector retrieval. ArXiv, abs/2304.01982. Available at: https://arxiv.org/abs/2304.01982 .\\n17. TensorFlow, 2021, TensorFlow hub, a model zoo with several easy to use pre-trained models. Available \\nat: https://tfhub.dev/.\\n18. Zhang, W., Xiong, C., & Zhao, H., 2023, Introducing BigQuery text embeddings for NLP tasks.  \\nGoogle Cloud Blog. Available at: https://cloud.google.com/blog/products/data-analytics/introducing  \\n- bigquery-text-embeddings .\\n19. Google Cloud, 2024, Get multimodal embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings .\\n20. Pinecone, 2024, IT Threat Detection. [online] Available at: https://docs.pinecone.io/docs/it-threat-detection .\\n21. Cai, H., Zheng, V. W., & Chang, K. C., 2020, A survey of algorithms and applications related with graph \\nembedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 49, 'page_label': '50'}, page_content='embedding. In Proceedings of the 29th ACM International Conference on Information & Knowledge \\nManagement. Available at: https://dl.acm.org/doi/10.1145/3444370.3444568 .\\n22. Cai, H., Zheng, V. W., & Chang, K. C., 2017, A comprehensive survey of graph embedding: problems, \\ntechniques and applications. ArXiv, abs/1709.07604. Available at: https://arxiv.org/pdf/1709.07604.pdf.\\n23. Hamilton, W. L., Ying, R. & Leskovec, J., 2017, Inductive representation learning on large graphs.  \\nIn Advances in Neural Information Processing Systems 30. Available at:  \\nhttps://cs.stanford.edu/people/jure/pubs/graphsage -nips17.pdf .\\n24. Dong, Z., Ni, J., Bikel, D. M., Alfonseca, E., Wang, Y., Qu, C. & Zitouni, I., 2022, Exploring dual encoder \\narchitectures for question answering. ArXiv, abs/2204.07120. Available at: https://arxiv.org/abs/2204.07120 .\\n25. Google Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 49, 'page_label': '50'}, page_content='25. Google Cloud, 2021, Vertex AI Generative AI: Tune Embeddings. Available at:  \\nhttps://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-embeddings .'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 50, 'page_label': '51'}, page_content='Embeddings & Vector Stores\\n51\\nSeptember 2024\\n26. TensorFlow, 2021, TensorFlow Models: NLP, Fine-tune BERT. Available at:  \\nhttps://www.tensorflow.org/tfmodels/nlp/fine_tune_bert .\\n27. Matsui, Y., 2020, Survey on approximate nearest neighbor methods. ACM Computing Surveys (CSUR), 53(6), \\nArticle 123. Available at: https://wangzwhu.github.io/home/file/acmmm-t-part3-ann.pdf .\\n28. Friedman, J. H., Bentley, J. L. & Finkel, R. A., 1977, An algorithm for finding best matches in logarithmic \\nexpected time. ACM Transactions on Mathematical Software (TOMS), 3(3), pp. 209-226. Available at:  \\nhttps://dl.acm.org/doi/pdf/10.1145/355744.355745 .\\n29. Scikit-learn, 2021, Scikit-learn, a library for unsupervised and supervised neighbors-based learning methods. \\nAvailable at: https://scikit-learn.org/.\\n30. lshashing, 2021, An open source python library to perform locality sensitive hashing. Available at:  \\nhttps://pypi.org/project/lshashing/ .'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 50, 'page_label': '51'}, page_content='30. lshashing, 2021, An open source python library to perform locality sensitive hashing. Available at:  \\nhttps://pypi.org/project/lshashing/ .\\n31. Malkov, Y. A., Yashunin, D. A., 2016, Efficient and robust approximate nearest neighbor search using \\nhierarchical navigable small world graphs. ArXiv, abs/1603.09320. Available at:  \\nhttps://arxiv.org/pdf/1603.09320.pdf .\\n32. Google Research, 2021, A library for fast ANN by Google using the ScaNN algorithm. Available at:  \\nhttps://github.com/google-research/google-research/tree/master/scann .\\n33. Guo, R., Zhang, L., Hinton, G. & Zoph, B., 2020, Accelerating large-scale inference with anisotropic vector \\nquantization. ArXiv, abs/1908.10396. Available at: https://arxiv.org/pdf/1908.10396.pdf .\\n34. TensorFlow, 2021, TensorFlow Recommenders, an open source library for building ranking & recommender \\nsystem models. Available at: https://www.tensorflow.org/recommenders .'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 50, 'page_label': '51'}, page_content='system models. Available at: https://www.tensorflow.org/recommenders .\\n35. Google Cloud, 2021, Vertex AI Vector Search, Google Cloud’s high-scale low latency vector database. \\nAvailable at: https://cloud.google.com/vertex-ai/docs/vector-search/overview .\\n36. Elasticsearch, 2021, Elasticsearch: a RESTful search and analytics engine. Available at:  \\nhttps://www.elastic.co/elasticsearch/ .\\n37. Pinecone, 2021, Pinecone, a commercial fully managed vector database. Available at:  \\nhttps://www.pinecone.io .\\n38. pgvector, 2021, Open Source vector similarity search for Postgres. Available at:  \\nhttps://github.com/pgvector/pgvector.\\n39. Weaviate, 2021, Weaviate, an open source vector database. Available at: https://weaviate.io/.'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 51, 'page_label': '52'}, page_content='Embeddings & Vector Stores\\n52\\nSeptember 2024\\n40. ChromaDB, 2021, ChromaDB, an open source vector database. Available at: https://www.trychroma.com/ .\\n41. LangChain, 2021.,LangChain, an open source framework for developing applications powered by language \\nmodel. Available at: https://langchain.com .')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfVthAk5SBpo"
      },
      "source": [
        "# **Insert VectorDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VLgxVbSZSEk9"
      },
      "outputs": [],
      "source": [
        "vector_store = Chroma(\n",
        "    collection_name=\"pdf_data\",\n",
        "    embedding_function=embeddings,\n",
        "    persist_directory=\"./chroma_db\",  # Where to save data locally, remove if not necessary\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ite6of4lGe6F",
        "outputId": "a19e5d7e-7867-4855-ce79-4efe0486ae32"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['f0cdbd9f-3e53-4e1f-9ff2-20f02703077f',\n",
              " 'bbb0522a-ba61-49f0-805c-59b6e0b65170',\n",
              " 'a98a44a9-69f3-41e7-b937-771e30fab06a',\n",
              " '0fb59854-2e73-486e-b9c3-5632d90e8dff',\n",
              " '1eab5c21-319c-4620-89a5-679d37729bf5',\n",
              " 'cddb1701-9b7d-4d32-9953-fa15da8163ee',\n",
              " 'dcdc89f8-b32e-4c8f-a185-9f1b493532ee',\n",
              " '86b7c75f-95bc-4d19-87f1-83902b02ef2c',\n",
              " 'b7d5934e-763b-4b55-b6f4-77d671cf6eff',\n",
              " '4b4ae806-a0e0-460d-ad11-651398a00444',\n",
              " '068fca3d-ba40-4160-9267-22c9ade98df7',\n",
              " '3d779163-aff7-4aa6-8958-f0d4dca7883e',\n",
              " 'ec5cf3d9-9eaa-498d-9a6c-2cb70adbab5f',\n",
              " '1ace247d-2eae-4b1f-8fcb-539ca213cb00',\n",
              " 'ccbbe882-a3d6-44b6-bc6b-fe9b2b9e375a',\n",
              " '5f75e36d-291a-4311-9dfd-91ffdc6b4b99',\n",
              " '2498bf95-e983-4238-a711-e21acbb75296',\n",
              " 'd0b4dd14-7bf8-4530-a0fc-99eb915a4d38',\n",
              " '092984b2-ebb6-412d-8d4e-2c4abafe583b',\n",
              " 'fa8182e5-60f6-4174-a67c-7efc1963018c',\n",
              " 'b09ab025-f993-4336-84c7-699adbf2c394',\n",
              " '1731b62b-9d21-4429-be26-ef0e164c15ea',\n",
              " '1289976e-1032-454f-91fe-17f8909dc059',\n",
              " 'b170bf4f-b43c-4eba-8af9-8d2544dc09c6',\n",
              " '23e515bc-f00a-41c7-808c-ab86e89eb090',\n",
              " 'ade89b15-482a-4f04-baa0-d144ff656f77',\n",
              " '9a55abf5-432f-4f30-8e60-6d6ef860ce2a',\n",
              " '3eb818aa-895e-4fd2-8e0b-557f691b179a',\n",
              " 'bde00470-d27e-46df-a83e-0a2ca5269a4e',\n",
              " '42440499-654e-443d-9fba-a48dd0e2b541',\n",
              " 'bf11fd65-93fe-4bce-843e-dcea74e3876f',\n",
              " '9192dc59-cd36-43b9-a285-9250c8d23d5e',\n",
              " '027a143b-fc64-4139-a36b-c94b67d2329a',\n",
              " 'ac3ad90a-5224-4351-8457-308762d464f1',\n",
              " '1c7eff35-9749-4821-af48-32bd47e2f572',\n",
              " '83ed5267-06c0-4d51-bc78-cb28b4472f4a',\n",
              " '640267d7-cdc6-4f8b-81c1-1516441bfb3a',\n",
              " '03b0874e-cdab-458f-b9a1-74b5da87cce1',\n",
              " '7f25bbd0-c0f2-4bd7-a481-215aaa14b1df',\n",
              " 'fe259055-cf0c-44f2-a800-591256e62f4e',\n",
              " 'c336b256-5cb8-4f29-90a9-2e0ff270ed08',\n",
              " '837d79fc-fbd8-4e1c-8e3d-233f5ebe0749',\n",
              " 'b69e6a18-f4a5-4c5e-b4b4-9586024f2f14',\n",
              " '163804e5-235a-45bf-b4ff-4369b2a16ef4',\n",
              " '9e07528c-3e3b-4b6c-a86b-f5c2d517b34b',\n",
              " 'c9ad8b54-7d0a-4b99-8602-1fa4d6ac57d2',\n",
              " '338c810a-dd11-4399-a808-07aab61b82c9',\n",
              " '015ac9de-b18f-42ed-9cf1-05d511e47735',\n",
              " 'b1b66069-ef0c-42a4-8e38-518fa8bc8627',\n",
              " 'd17d6cf0-64a5-4fbf-a307-df8469148d5d',\n",
              " 'ed6fb9a6-c54a-412f-9dd4-fc0ad687fd5c',\n",
              " '29f729ca-cd11-4838-a132-483a0d6640a0',\n",
              " '8c47edd4-7388-4dbf-92d2-4762da13b6d3',\n",
              " 'ebc0413e-8895-4ad7-9fc5-22cd1d1633be',\n",
              " 'a6b07d1b-8dbc-4a88-abf9-65484aee0fe3',\n",
              " '48fde50d-80ec-4204-a12d-0fced955c934',\n",
              " '963e6afa-77ec-47b7-b154-3f51b673843e',\n",
              " '81b63994-91e8-4746-a469-a15430064b6c',\n",
              " 'edf85c09-5fa3-4e9f-bbca-d076d50fcf6a',\n",
              " 'afd99c04-e280-44f7-a15f-9a0fcc9485c0',\n",
              " '71cc73a6-3c25-47c5-aa96-3ca7986adbc8',\n",
              " 'c2de6e76-565a-47d0-9206-c073e2ae1723',\n",
              " '0d66ba9e-236a-4eb7-b91b-3a8ff7900282',\n",
              " '6ff54f78-20c1-4eaf-8e5f-878b84044db4',\n",
              " '4917f853-ac47-4ae9-ac8f-d81cd424eda4',\n",
              " '57147c24-4da6-49f9-aa92-1c8b97756bca',\n",
              " '1a4f70c5-0404-495b-ad16-722a9d0aa2af',\n",
              " 'f039c7ed-cb2a-45be-ae29-71639906cb9c',\n",
              " '521532e4-21d7-4a02-936c-1f0a21724ba0',\n",
              " 'dc001dae-db4e-4642-b188-407d3d500bb1',\n",
              " '06cb2d03-3430-4e7f-9846-afcacfeb51d5',\n",
              " '8c80a4c8-e1ac-4935-96c3-73a067465eea',\n",
              " 'f81c2e6d-d284-4281-9de8-71c172681933',\n",
              " 'b0b4f224-c712-438c-a5cc-571dca5ba768',\n",
              " 'a53c6735-4da6-4048-9c9b-cef854e32b73',\n",
              " '66f71330-7293-4cbb-bfeb-ce8b7f2109ad',\n",
              " '4f3f1d4c-8db6-4f39-aa5d-f3b400ef0479',\n",
              " '3ea59f4b-cf58-4980-bc6c-4087d9983ec3',\n",
              " '0fac28ac-7ef3-4461-aa5f-eb2072e8bb3e',\n",
              " 'f0d2a0bc-9acb-41f8-a4fe-4b578adaf9c5',\n",
              " '7ba07b55-ca42-473d-80d9-e973057824a3',\n",
              " 'fa6ad84b-a5ec-4b05-9002-e8bd57175584',\n",
              " '4f7b6fa5-06be-48e2-8f1f-d0df7a9e0af9',\n",
              " '1ad8a6a7-f65e-4874-a0e0-06c59e2ac3b7',\n",
              " '971efed9-e6b3-43e1-b8d4-7cfe904439ff',\n",
              " 'b193f4fc-48ce-44b4-b8a3-59cc3516f4a9',\n",
              " 'cbf8fe00-da10-477e-8c73-0d6d48220d1d',\n",
              " '9f53c57d-c15c-4751-8529-7904e05f0bde',\n",
              " '089e9103-5d6d-4e71-9f14-df0df1b5ee23',\n",
              " 'e1da0821-0559-4ebd-8092-aa5c63a3b772',\n",
              " '46f6ca3d-a918-4224-a167-5e88a94cc344',\n",
              " 'ca091792-30bd-4e9f-b212-9b0cd2467a77',\n",
              " 'dda3793d-4f11-4e3e-aa4c-bda500f54449',\n",
              " '1d79dcac-1ca6-4351-92fe-35a3af64b5c5',\n",
              " '0d0f5cca-701d-492b-91a1-5d21adf5aaf1',\n",
              " 'a7619e92-abc2-4dc4-8d8d-ecb487894e14',\n",
              " '28feda56-29b7-42a6-af25-461ffd10a608',\n",
              " '31465344-5407-4d7b-b3ca-3d1f6dcb405e',\n",
              " 'ec780bd5-f2ac-4cd4-8468-5ed96f253033',\n",
              " '1aa4edef-4888-42e9-bd59-7e87fd0d9fca',\n",
              " '2863bd8f-9e1c-45a6-983c-505f98bfc789',\n",
              " 'ab176165-bd7d-4a70-be17-a5416e594f3a',\n",
              " '089b6cb3-13ce-40af-909c-5a95ccb43429']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.add_documents(documents=texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ARJhSnJSE9f"
      },
      "source": [
        "# **Set Up BM25**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KLUJmdspSI2v"
      },
      "outputs": [],
      "source": [
        "bm25_retriever = BM25Retriever.from_documents(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9hNottlSKfN"
      },
      "source": [
        "# **Retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYuCn-ssGdhC",
        "outputId": "2cde6f7b-43d6-401e-948c-28f1f7ba3dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type of bm25 <class 'langchain_community.retrievers.bm25.BM25Retriever'>\n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever()\n",
        "bm25_retriever.k = 2  # Retrieve top 2 results\n",
        "\n",
        "print(\"type of bm25\", type(bm25_retriever))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HR_qbARSGl4L"
      },
      "outputs": [],
      "source": [
        "# Initialize the ensemble retriever\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, retriever], weights=[0.2, 0.8]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7N0XVQaSNVv"
      },
      "source": [
        "# **Chain - Query**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgToeYwtSQfl",
        "outputId": "80ffa613-2c1c-4e42-bcd5-0498f5d0099d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-7e3b8e423e1d>:6: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = ensemble_retriever.get_relevant_documents(query)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'contoh.pdf', 'page': 5, 'page_label': '6'}, page_content='Embeddings & Vector Stores\\n6\\nSeptember 2024\\n• Efficient Management: Techniques for storing, retrieving, and searching vast collections \\nof embeddings.\\n• Vector Databases: Specialized systems for managing and querying embeddings, \\nincluding practical considerations for production deployment.\\n• Real-World Applications: Concrete examples of how embeddings and vector databases \\nare combined with large language models (LLMs) to solve real-world problems.\\nThroughout the whitepaper, code snippets provide hands-on illustrations of key concepts.\\nWhy embeddings are important\\nIn essence, embeddings are numerical representations of real-world data such as text, \\nspeech, image, or videos. They are expressed as low-dimensional vectors where the \\ngeometric distances of two vectors in the vector space is a projection of the relationships \\nbetween the two real-world objects that the vectors represent. In other words they help you'),\n",
              " Document(metadata={'source': 'contoh.pdf', 'page': 2, 'page_label': '3'}, page_content='Introduction 5\\nWhy embeddings are important 6\\n Types of embeddings 9\\n  Text embeddings 9\\n   Word embeddings 11\\n   Document embeddings 15\\n    Shallow BoW models 16\\n    Deeper pretrained large language models 18\\n  Image & multimodal embeddings 22\\n  Structured data embeddings 24\\n   General structured data  24\\n   User/item structured data 25\\n  Graph embeddings 25\\n Training Embeddings 26\\nTable of contents'),\n",
              " Document(id='b7d5934e-763b-4b55-b6f4-77d671cf6eff', metadata={'page': 6, 'page_label': '7', 'source': 'contoh.pdf'}, page_content='to project these objects into the same vector space that is both compact and informative. \\nIdeally, such a representation would capture as much of the original object’s characteristics \\nas possible. An embedding refers to the projected vector of an object from an input space to \\na relatively low-dimensional vector space. Each vector is a list of floating point numbers.'),\n",
              " Document(id='4b4ae806-a0e0-460d-ad11-651398a00444', metadata={'page': 7, 'page_label': '8', 'source': 'contoh.pdf'}, page_content='Embeddings & Vector Stores\\n8\\nSeptember 2024\\nFigure 1. Projecting objects/content into a joint vector space with semantic meaning\\nIdeally the embeddings are created so they place objects with similar semantic properties \\ncloser in the embedding space (a low-dimensional vector space where items can be \\nprojected). The embeddings can then be used as a condensed, meaningful input in \\ndownstream applications. For example, you can use them as features for ML models, \\nrecommender systems, search engines, and many more. So your data not only gets a \\ncompact numerical representation, but this representation also preserves the semantic \\nmeanings for a specific task or across a variety of tasks. The fact that these representations \\nare task-specific means you can generate different embeddings for the same object, \\noptimized for the task at hand.')]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example customer query\n",
        "query = \" Why embeddings are important ?\"\n",
        "\n",
        "\n",
        "# Retrieve relevant documents/products\n",
        "docs = ensemble_retriever.get_relevant_documents(query)\n",
        "\n",
        "docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bZnU4dKfGpTk"
      },
      "outputs": [],
      "source": [
        "def generate_answers(query: str, llm = model_llm, retriever = ensemble_retriever):\n",
        "\n",
        "    qa_chain = RetrievalQA.from_llm(\n",
        "                llm,\n",
        "                retriever = retriever\n",
        "            )\n",
        "    response = qa_chain.invoke(query)\n",
        "\n",
        "    # return response[\"result\"]\n",
        "    return response[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4hkadRO1KBwu"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-xgUxg2JpOK",
        "outputId": "530120a4-4f57-4c07-8b51-01c2ce16807e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Embeddings are important because they provide a numerical representation of '\n",
            " 'real-world data, such as text, speech, images, or videos, as low-dimensional '\n",
            " 'vectors. These vectors capture the relationships between the objects they '\n",
            " 'represent, allowing for a compact and informative projection of the original '\n",
            " \"object's characteristics.\\n\"\n",
            " '\\n'\n",
            " 'Ideally, embeddings are created to place objects with similar semantic '\n",
            " 'properties closer together in the embedding space, making them a condensed '\n",
            " 'and meaningful input for downstream applications. This representation '\n",
            " 'preserves the semantic meanings for a specific task or across various tasks, '\n",
            " 'enabling the generation of different embeddings for the same object, '\n",
            " 'optimized for the task at hand.\\n'\n",
            " '\\n'\n",
            " 'Embeddings are crucial because they facilitate a range of applications, '\n",
            " 'including:\\n'\n",
            " '\\n'\n",
            " '1. **Feature extraction**: Embeddings can be used as features for machine '\n",
            " 'learning models, allowing for more efficient and accurate training.\\n'\n",
            " '2. **Recommender systems**: Embeddings can be used to recommend items or '\n",
            " 'products based on user behavior and preferences.\\n'\n",
            " '3. **Search engines**: Embeddings can improve search engine ranking and '\n",
            " 'retrieval by capturing the semantic meaning of search queries.\\n'\n",
            " '4. **Data analysis**: Embeddings can help identify patterns and '\n",
            " 'relationships within large datasets, enabling more effective '\n",
            " 'decision-making.\\n'\n",
            " '\\n'\n",
            " 'Overall, embeddings provide a powerful tool for data representation and '\n",
            " 'analysis, enabling a wide range of applications and use cases.')\n"
          ]
        }
      ],
      "source": [
        "pprint(generate_answers('Why embeddings are important'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMIbmL2uJ05q",
        "outputId": "91f75b1f-70bc-4d43-f60d-57c61ec348e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('According to the provided context, embeddings can be of various forms and '\n",
            " 'are used for different types of data. Some standard techniques used for '\n",
            " 'different types of data include:\\n'\n",
            " '\\n'\n",
            " '1. **Text embeddings**: These are used for natural language processing (NLP) '\n",
            " 'and are often used to embed the meaning of natural language in machine '\n",
            " 'learning for processing in various downstream applications, such as text '\n",
            " 'generation, classification, sentiment analysis, and more.\\n'\n",
            " '\\n'\n",
            " '   - **Token/word embeddings**: These are used to represent words or tokens '\n",
            " 'in a vector space.\\n'\n",
            " '   - **Document embeddings**: These are used to represent entire documents '\n",
            " 'in a vector space.\\n'\n",
            " '\\n'\n",
            " '2. **Image embeddings**: These are used for image processing and can be used '\n",
            " 'for tasks such as image classification, object detection, and image '\n",
            " 'generation.\\n'\n",
            " '\\n'\n",
            " \"It's worth noting that these are not the only types of embeddings, and the \"\n",
            " 'context mentions that embeddings can be used for various forms of data, '\n",
            " 'including speech and videos.')\n"
          ]
        }
      ],
      "source": [
        "pprint(generate_answers('what is types of embeddings'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLaajoPlKssb",
        "outputId": "6d839c4a-99bd-4d71-a101-5e4d0168b1cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('Based on the provided context, Vector Search refers to a technique used in '\n",
            " 'machine learning and natural language processing (NLP) to efficiently query '\n",
            " 'and retrieve relevant data based on its semantic meaning. This is achieved '\n",
            " 'by representing data as numerical vectors, known as embeddings, which can be '\n",
            " 'compared and searched using various algorithms.\\n'\n",
            " '\\n'\n",
            " 'The important thing about Vector Search is that it allows for:\\n'\n",
            " '\\n'\n",
            " '1. **Efficient querying**: Vector Search enables fast and efficient querying '\n",
            " 'of large datasets, making it suitable for real-time applications and complex '\n",
            " 'analysis.\\n'\n",
            " '2. **Semantic search**: By capturing the semantic meaning of data, Vector '\n",
            " 'Search can retrieve relevant data even when the query is not an exact '\n",
            " 'match.\\n'\n",
            " '3. **Scalability**: Vector Search can handle large datasets and scale to '\n",
            " 'meet the demands of modern applications.\\n'\n",
            " '4. **Flexibility**: Vector Search can be used in various domains, including '\n",
            " 'but not limited to, natural language processing, computer vision, and '\n",
            " 'recommender systems.\\n'\n",
            " '\\n'\n",
            " 'Some key benefits of Vector Search include:\\n'\n",
            " '\\n'\n",
            " '1. **Improved search accuracy**: Vector Search can retrieve more relevant '\n",
            " 'results by capturing the semantic meaning of data.\\n'\n",
            " '2. **Faster search times**: Vector Search can significantly reduce search '\n",
            " 'times, making it suitable for real-time applications.\\n'\n",
            " '3. **Better handling of ambiguity**: Vector Search can handle ambiguous '\n",
            " 'queries and retrieve relevant results even when the query is not an exact '\n",
            " 'match.\\n'\n",
            " '\\n'\n",
            " 'Some popular applications of Vector Search include:\\n'\n",
            " '\\n'\n",
            " '1. **Recommendation systems**: Vector Search can be used to retrieve '\n",
            " 'relevant product recommendations based on user behavior and preferences.\\n'\n",
            " '2. **Search engines**: Vector Search can be used to improve search engine '\n",
            " 'results by capturing the semantic meaning of search queries.\\n'\n",
            " '3. **Natural language processing**: Vector Search can be used to improve '\n",
            " 'text classification, sentiment analysis, and other NLP tasks.\\n'\n",
            " '\\n'\n",
            " 'Some popular algorithms used in Vector Search include:\\n'\n",
            " '\\n'\n",
            " '1. **ScaNN**: A fast and efficient algorithm for approximate nearest '\n",
            " 'neighbor search.\\n'\n",
            " '2. **FAISS**: A library for efficient similarity search and clustering of '\n",
            " 'dense vectors.\\n'\n",
            " '3. **LSH**: A library for efficient similarity search and clustering of '\n",
            " 'sparse vectors.\\n'\n",
            " '4. **HNSW**: A library for efficient similarity search and clustering of '\n",
            " 'dense vectors.\\n'\n",
            " '\\n'\n",
            " 'Overall, Vector Search is a powerful technique for efficiently querying and '\n",
            " 'retrieving relevant data based on its semantic meaning, and it has many '\n",
            " 'important applications in machine learning and natural language processing.')\n"
          ]
        }
      ],
      "source": [
        "pprint(generate_answers('what is vectorsearch and the important thing about it'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00c46469b578408aaa0dbbe6fc660a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f34c7fcf71a04f3187040668600b4c91",
              "IPY_MODEL_25782e88330049f99833a1248608551c",
              "IPY_MODEL_59c02a0a307a4474ad112750fb8723f7"
            ],
            "layout": "IPY_MODEL_89fc44c2d5e14c7cab1017ea7ef97468"
          }
        },
        "025ed361d0bd4f789a90c5990044331d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06dbb95f22c04221826c10e160660624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b6b60c18373447a80e2c1bb63bf97d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "107427356dd64cc0926d5ffc9aa80cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895ae74a00004c25b0f016dec3ede2bb",
            "placeholder": "​",
            "style": "IPY_MODEL_493b11052454443d829d980029800e4a",
            "value": "model.safetensors: 100%"
          }
        },
        "161cab54a58647eb8a881a3f01fd3d94": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f6ec74cc83420b8cdb52f2bcd8c927": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a243c89895c48df958b5b2b920874e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f14f42251a2439bb2ab3f2a4b4800c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f2bceea54ff4d8aa6283c4e6c41ddc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8371984b116149429a4506becb3fbae6",
              "IPY_MODEL_90be61d6650d496584140672f5bbdafb",
              "IPY_MODEL_a83eced765f14741a1347c91444e94e7"
            ],
            "layout": "IPY_MODEL_7acb54d64b124184b143e644676f6e38"
          }
        },
        "215921d424234a2eb4a27d0e4dabcbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b0f69ec88ab4bfa97fe1969579344c5",
            "placeholder": "​",
            "style": "IPY_MODEL_4340306ab97e420eb658cd0f5607ee9a",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "23b3ebf7ee6c417cb294c62e9795e224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a243c89895c48df958b5b2b920874e7",
            "placeholder": "​",
            "style": "IPY_MODEL_c833c4c58ff4475d9ec4781fa91720cd",
            "value": "tokenizer.json: 100%"
          }
        },
        "25782e88330049f99833a1248608551c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d07b7f893b26422ca85d4554505283c6",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60e8326217b740899977d8d189315a65",
            "value": 116
          }
        },
        "269c5f04b6c04b08b16abf3e1b807499": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c3ca12ce8a48fea746e9c8b2c82799": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "280b9d38456448e2b37518918b763367": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2958caea11244e22b60af5541727403f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bdcfa7743e643cc982d4ed61ae4c970": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_107427356dd64cc0926d5ffc9aa80cc6",
              "IPY_MODEL_416115c537e04a88895e6a2e8152b418",
              "IPY_MODEL_578395115d2b458192e228de5357eb55"
            ],
            "layout": "IPY_MODEL_9e26c5ee0f79499d9075008d460cf631"
          }
        },
        "2eb05b235bcd45d491f7a331382e70c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31e5baddb7da4b6e93eae9b578bef37e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333de635f4f44022a04b49a7a7155027": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_626d64057109443f8c8a1ff4627da87a",
            "placeholder": "​",
            "style": "IPY_MODEL_06dbb95f22c04221826c10e160660624",
            "value": "vocab.txt: 100%"
          }
        },
        "35c2621e8ac6422eb201f7616fa94bdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d4b78fb7e64dce9ad10bd9ad02f428": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d52bb28ab84ac0a7023ed1f23b9a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3946f007d18f494ba1b4ca2cc1f4dc95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39ee33bcfda2446d9dd57736eb6ede85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a5ebf33451443709348180a2608a779": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b0f69ec88ab4bfa97fe1969579344c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dce8e53dba444509931cd9023722480": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_269c5f04b6c04b08b16abf3e1b807499",
            "placeholder": "​",
            "style": "IPY_MODEL_b03862c25c4c41debf9184e8585160a7",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 521kB/s]"
          }
        },
        "3eb8c144062f4fea85b4080197396c07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3946f007d18f494ba1b4ca2cc1f4dc95",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79ebd99dd79845899cf32cd85d5d5b59",
            "value": 190
          }
        },
        "4093d36d2904437ba502385ee7f60268": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40dec5ec080e46d8ab6fd83860f2c8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "416115c537e04a88895e6a2e8152b418": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebff0a974f9d4efc836c5bfdec402de5",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6dca5c75547d4f78bcd5831c9f51b389",
            "value": 90868376
          }
        },
        "4340306ab97e420eb658cd0f5607ee9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4628912565dc409eb484ac59d8f0dffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47d20c260a25404c9e8b5a68d043a372": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "493b11052454443d829d980029800e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a1c91c87b0344329c43ef47f3f650c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8707bc83858420e957d8c42ebbe2e3b",
            "placeholder": "​",
            "style": "IPY_MODEL_8e3d65dc8d6741e884b8985678895201",
            "value": "config.json: 100%"
          }
        },
        "4a2be97e4a6447be9f10057264b78a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a45a5dc16da400c9c0407cda429a28b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c52e5dbd5cb447fa8a56a988054882f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed8f3049cf94987ad1cb10118d1f7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_025ed361d0bd4f789a90c5990044331d",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb1c89c7910f4047934a538690a98491",
            "value": 112
          }
        },
        "51a0cc888f18415ba53d1eab62518e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e5baddb7da4b6e93eae9b578bef37e",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8929fb1acf144d27ab7d592fb6e99f05",
            "value": 350
          }
        },
        "558a8d20495e4b548f0ad5ec323aff7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2958caea11244e22b60af5541727403f",
            "placeholder": "​",
            "style": "IPY_MODEL_ea91635d846b4dbabe565b97aaf3aee3",
            "value": " 612/612 [00:00&lt;00:00, 35.3kB/s]"
          }
        },
        "578395115d2b458192e228de5357eb55": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35c2621e8ac6422eb201f7616fa94bdd",
            "placeholder": "​",
            "style": "IPY_MODEL_4628912565dc409eb484ac59d8f0dffb",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 222MB/s]"
          }
        },
        "59c02a0a307a4474ad112750fb8723f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ce1e6d73a674b7d950d52999a8ca98e",
            "placeholder": "​",
            "style": "IPY_MODEL_67a734d3e42445c381e30f1462fbbd5a",
            "value": " 116/116 [00:00&lt;00:00, 9.59kB/s]"
          }
        },
        "5a933bfa70094f07a2c7efa2fbd952b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60e8326217b740899977d8d189315a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61aa8fce7a8945d59b6f85b3f5d056e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4ee15a82dfd4184b6258bd697851c71",
            "placeholder": "​",
            "style": "IPY_MODEL_35d4b78fb7e64dce9ad10bd9ad02f428",
            "value": "README.md: 100%"
          }
        },
        "626d64057109443f8c8a1ff4627da87a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664c3d16230a48a9b29104c41b6c398e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67a734d3e42445c381e30f1462fbbd5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a8af57ce8d7487785477a98ad70bd71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a5ebf33451443709348180a2608a779",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d93c17258807476eaad0f2ab5fcee4f2",
            "value": 53
          }
        },
        "6ce1e6d73a674b7d950d52999a8ca98e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dca5c75547d4f78bcd5831c9f51b389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f1b4c6d5ba74dcfb3bf517bb3e4ee4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7543674f08fa470fb2613ca3442d62f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "783556b30c6144259d4b896365eb5ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78dafdba36a14b1c90d0ca38524b8f12",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a933bfa70094f07a2c7efa2fbd952b3",
            "value": 612
          }
        },
        "78dafdba36a14b1c90d0ca38524b8f12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7966662e54394544b1e2514913b3cb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79ebd99dd79845899cf32cd85d5d5b59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7acb54d64b124184b143e644676f6e38": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c13b23ab81c4b98b5766bab459f52fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6c72ec6af54e8885adc99619840352": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80cd948c7fde4f1da1346f256e868501": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_333de635f4f44022a04b49a7a7155027",
              "IPY_MODEL_fa3237618d024e27b2e2a9ff40064892",
              "IPY_MODEL_9bdb7406f57c4deeb4f663c3c58a6b92"
            ],
            "layout": "IPY_MODEL_7f6c72ec6af54e8885adc99619840352"
          }
        },
        "8371984b116149429a4506becb3fbae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c1c9e62a01748f386a6b8e1ec91135f",
            "placeholder": "​",
            "style": "IPY_MODEL_37d52bb28ab84ac0a7023ed1f23b9a9a",
            "value": "modules.json: 100%"
          }
        },
        "84023bde639149c59f12b8aaea0cf534": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b6a5ef8189411bac4fee8f38e34b4c",
            "placeholder": "​",
            "style": "IPY_MODEL_da272f13f037431f9ae9fc63af7f3fce",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "8426aeaefca047269d5203d205236b76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d565218f0bda47e590ca81850496f795",
              "IPY_MODEL_6a8af57ce8d7487785477a98ad70bd71",
              "IPY_MODEL_99e25b9814e34245ac47e6c67fdb51cf"
            ],
            "layout": "IPY_MODEL_d2a03ad1d93645b6baee7b6f56c195b3"
          }
        },
        "869a6d28f66e4aaebaa555f1706f78f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8929fb1acf144d27ab7d592fb6e99f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "895ae74a00004c25b0f016dec3ede2bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89ac705d9394495dbb0ec47430e1e2a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89fc44c2d5e14c7cab1017ea7ef97468": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bfd0118c89f4c13851fba980e250641": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c1c9e62a01748f386a6b8e1ec91135f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca85c76ce0e4ad58df32be85df747f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23b3ebf7ee6c417cb294c62e9795e224",
              "IPY_MODEL_938445b86b51426abdb663e2bf1d6136",
              "IPY_MODEL_c8dbe2b025104941917877c61ec383cf"
            ],
            "layout": "IPY_MODEL_89ac705d9394495dbb0ec47430e1e2a5"
          }
        },
        "8e3d65dc8d6741e884b8985678895201": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f1474202abd4559bf95f16087b5a4c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84023bde639149c59f12b8aaea0cf534",
              "IPY_MODEL_3eb8c144062f4fea85b4080197396c07",
              "IPY_MODEL_907f5bcb83b64074991a3f38e3844267"
            ],
            "layout": "IPY_MODEL_8bfd0118c89f4c13851fba980e250641"
          }
        },
        "8f27d6d6ffa64dbab685b7b348662d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1612d07c47a419e8bd7ed1fa0656a31",
            "placeholder": "​",
            "style": "IPY_MODEL_2eb05b235bcd45d491f7a331382e70c8",
            "value": " 112/112 [00:00&lt;00:00, 9.38kB/s]"
          }
        },
        "907f5bcb83b64074991a3f38e3844267": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_280b9d38456448e2b37518918b763367",
            "placeholder": "​",
            "style": "IPY_MODEL_869a6d28f66e4aaebaa555f1706f78f5",
            "value": " 190/190 [00:00&lt;00:00, 10.9kB/s]"
          }
        },
        "90be61d6650d496584140672f5bbdafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ac313c326324d15b5ba259879381fac",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c14f850a6d3a4d58885f677a30a93906",
            "value": 349
          }
        },
        "938445b86b51426abdb663e2bf1d6136": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c52e5dbd5cb447fa8a56a988054882f",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40dec5ec080e46d8ab6fd83860f2c8c2",
            "value": 466247
          }
        },
        "99e25b9814e34245ac47e6c67fdb51cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c3ca12ce8a48fea746e9c8b2c82799",
            "placeholder": "​",
            "style": "IPY_MODEL_f1f526b078ee4d438ac7a45639eef6a6",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.81kB/s]"
          }
        },
        "9ac313c326324d15b5ba259879381fac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bdb7406f57c4deeb4f663c3c58a6b92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c13b23ab81c4b98b5766bab459f52fe",
            "placeholder": "​",
            "style": "IPY_MODEL_6f1b4c6d5ba74dcfb3bf517bb3e4ee4c",
            "value": " 232k/232k [00:00&lt;00:00, 13.4MB/s]"
          }
        },
        "9d49c106db914063a933e10236f7e0bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e26c5ee0f79499d9075008d460cf631": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a027e8b1f49f4cd09d3dde6b54881df3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a586088bf63b4f9f80d898d3f0240885": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a83eced765f14741a1347c91444e94e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebe3f13acc464184b1bbc1c58d614122",
            "placeholder": "​",
            "style": "IPY_MODEL_47d20c260a25404c9e8b5a68d043a372",
            "value": " 349/349 [00:00&lt;00:00, 21.6kB/s]"
          }
        },
        "a8707bc83858420e957d8c42ebbe2e3b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b03862c25c4c41debf9184e8585160a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b198c5e0c4bc47c1b2964f0aefe33448": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1b6a5ef8189411bac4fee8f38e34b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14f850a6d3a4d58885f677a30a93906": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1612d07c47a419e8bd7ed1fa0656a31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c833c4c58ff4475d9ec4781fa91720cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8dbe2b025104941917877c61ec383cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b198c5e0c4bc47c1b2964f0aefe33448",
            "placeholder": "​",
            "style": "IPY_MODEL_7966662e54394544b1e2514913b3cb1c",
            "value": " 466k/466k [00:00&lt;00:00, 675kB/s]"
          }
        },
        "cb1c89c7910f4047934a538690a98491": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb666b1ef0904b33b5c0248ee1b524c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a2be97e4a6447be9f10057264b78a48",
            "placeholder": "​",
            "style": "IPY_MODEL_f80fc677b9d2480fb0e5a824a3f11032",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "d07b7f893b26422ca85d4554505283c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2a03ad1d93645b6baee7b6f56c195b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d33eed0297ae48c387638a6478579d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb666b1ef0904b33b5c0248ee1b524c3",
              "IPY_MODEL_51a0cc888f18415ba53d1eab62518e0d",
              "IPY_MODEL_edfaed4a47954ce9991b0b894a0dbf7c"
            ],
            "layout": "IPY_MODEL_f04524751ef44e3987420e2f870f0164"
          }
        },
        "d565218f0bda47e590ca81850496f795": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a45a5dc16da400c9c0407cda429a28b",
            "placeholder": "​",
            "style": "IPY_MODEL_39ee33bcfda2446d9dd57736eb6ede85",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "d93c17258807476eaad0f2ab5fcee4f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da272f13f037431f9ae9fc63af7f3fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "defe10fe7430422eb4efe4bdc50268f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_215921d424234a2eb4a27d0e4dabcbf1",
              "IPY_MODEL_4ed8f3049cf94987ad1cb10118d1f7be",
              "IPY_MODEL_8f27d6d6ffa64dbab685b7b348662d81"
            ],
            "layout": "IPY_MODEL_a027e8b1f49f4cd09d3dde6b54881df3"
          }
        },
        "e42b2ba9dede4206821ad2118dd994f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4ee15a82dfd4184b6258bd697851c71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7acc65659a54c8d9f97d808215b1460": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a1c91c87b0344329c43ef47f3f650c2",
              "IPY_MODEL_783556b30c6144259d4b896365eb5ade",
              "IPY_MODEL_558a8d20495e4b548f0ad5ec323aff7c"
            ],
            "layout": "IPY_MODEL_9d49c106db914063a933e10236f7e0bd"
          }
        },
        "e87279cf86994427b32d011e116ee537": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e42b2ba9dede4206821ad2118dd994f6",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b6b60c18373447a80e2c1bb63bf97d2",
            "value": 10659
          }
        },
        "ea91635d846b4dbabe565b97aaf3aee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebe3f13acc464184b1bbc1c58d614122": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebff0a974f9d4efc836c5bfdec402de5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edfaed4a47954ce9991b0b894a0dbf7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_161cab54a58647eb8a881a3f01fd3d94",
            "placeholder": "​",
            "style": "IPY_MODEL_1f14f42251a2439bb2ab3f2a4b4800c9",
            "value": " 350/350 [00:00&lt;00:00, 25.5kB/s]"
          }
        },
        "f04524751ef44e3987420e2f870f0164": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f526b078ee4d438ac7a45639eef6a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f34c7fcf71a04f3187040668600b4c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f6ec74cc83420b8cdb52f2bcd8c927",
            "placeholder": "​",
            "style": "IPY_MODEL_664c3d16230a48a9b29104c41b6c398e",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "f80fc677b9d2480fb0e5a824a3f11032": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa3237618d024e27b2e2a9ff40064892": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a586088bf63b4f9f80d898d3f0240885",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4093d36d2904437ba502385ee7f60268",
            "value": 231508
          }
        },
        "fb3f76fbe5e84b3697a91ac81c275c7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_61aa8fce7a8945d59b6f85b3f5d056e0",
              "IPY_MODEL_e87279cf86994427b32d011e116ee537",
              "IPY_MODEL_3dce8e53dba444509931cd9023722480"
            ],
            "layout": "IPY_MODEL_7543674f08fa470fb2613ca3442d62f7"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
